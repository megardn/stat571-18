---
title: "Modern Data Mining, HW 2 GROUP 18"
author:
- Diego G. Davila
- Margaret Gardner
- Joelle Bagautdinova
date: 'Due: 11:59 PM,  Sunday, 02/13'
output:
  html_document:
    code_folding: show
    highlight: haddock
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '4'
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=8, fig.height=4)
options(scipen = 0, digits = 3)  # controls base R output

# check if you have ISLR package, if not, install it
# if(!require('pacman', 'factoextra', 'AMR')) {
#   install.packages('pacman', 'factoextra', 'AMR')
# } # Joelle: this isn't working for me :/
# if(!require('reshape2')) {install.packages('reshape2')} # this will be useful later for plotting

if(!require('pacman')) {
  install.packages('pacman')
}
output_format <- ifelse(is.null(knitr::opts_knit$get("rmarkdown.pandoc.to")),
                        "text", knitr::opts_knit$get("rmarkdown.pandoc.to"))

pacman::p_load(ISLR, tidyverse, data.table, reshape2, AMR, factoextra, stargazer, ggpubr, irlba, ggrepel, GGally) # add the packages needed

```


\pagebreak

# Overview {-}

Principle Component Analysis is widely used in data exploration, dimension reduction, data visualization. The aim is to transform original data into uncorrelated linear combinations of the original data while keeping the information contained in the data. High dimensional data tends to show clusters in lower dimensional view. 

Clustering Analysis is another form of EDA. Here we are hoping to group data points which are close to each other within the groups and far away between different groups. Clustering using PC's can be effective. Clustering analysis can be very subjective in the way we need to summarize the properties within each group. 

Both PCA and Clustering Analysis are so called unsupervised learning. There is no response variables involved in the process. 

For supervised learning, we try to find out how does a set of predictors relate to some response variable of the interest. Multiple regression is still by far, one of the most popular methods. We use linear models as a working model for its simplicity and interpretability. It is important that we use domain knowledge as much as we can to determine the form of the response as well as the function format of the factors.


## Objectives

- PCA
- SVD
- Clustering Analysis
- Linear Regression

## Review materials

- Study Module 2: PCA
- Study Module 3: Clustering Analysis
- Study Module 4: Multiple regression

## Data needed

- `NLSY79.csv`
- `brca_subtype.csv`
- `brca_x_patient.csv`

# Case study 1: Self-seteem 

Self-esteem generally describes a person's overall sense of self-worthiness and personal value. It can play significant role in one's motivation and success throughout the life. Factors that influence self-esteem can be inner thinking, health condition, age, life experiences etc. We will try to identify possible factors in our data that are related to the level of self-esteem. 

In the well-cited National Longitudinal Study of Youth (NLSY79), it follows about 13,000 individuals and numerous individual-year information has been gathered through surveys. The survey data is open to public [here](https://www.nlsinfo.org/investigator/). Among many variables we assembled a subset of variables including personal demographic variables in different years, household environment in 79, ASVAB test Scores in 81 and Self-Esteem scores in 81 and 87 respectively. 

The data is store in `NLSY79.csv`.

Here are the description of variables:

**Personal Demographic Variables**

* Gender: a factor with levels "female" and "male"
* Education05: years of education completed by 2005
* HeightFeet05, HeightInch05: height measurement. For example, a person of 5'10 will be recorded as HeightFeet05=5, HeightInch05=10.
* Weight05: weight in lbs.
* Income87, Income05: total annual income from wages and salary in 2005. 
* Job87, Job05: job type in 1987 and 2005, including Protective Service Occupations, Food Preparation and Serving Related Occupations, Cleaning and Building Service Occupations, Entertainment Attendants and Related Workers, Funeral Related Occupations, Personal Care and Service Workers, Sales and Related Workers, Office and Administrative Support Workers, Farming, Fishing and Forestry Occupations, Construction Trade and Extraction Workers, Installation, Maintenance and Repairs Workers, Production and Operating Workers, Food Preparation Occupations, Setters, Operators and Tenders,  Transportation and Material Moving Workers
 
 
**Household Environment**
 
* Imagazine: a variable taking on the value 1 if anyone in the respondent’s household regularly read magazines in 1979, otherwise 0
* Inewspaper: a variable taking on the value 1 if anyone in the respondent’s household regularly read newspapers in 1979, otherwise 0
* Ilibrary: a variable taking on the value 1 if anyone in the respondent’s household had a library card in 1979, otherwise 0
* MotherEd: mother’s years of education
* FatherEd: father’s years of education
* FamilyIncome78

**Variables Related to ASVAB test Scores in 1981**

Test | Description
--------- | ------------------------------------------------------
AFQT | percentile score on the AFQT intelligence test in 1981 
Coding | score on the Coding Speed test in 1981
Auto | score on the Automotive and Shop test in 1981
Mechanic | score on the Mechanic test in 1981
Elec | score on the Electronics Information test in 1981
Science | score on the General Science test in 1981
Math | score on the Math test in 1981
Arith | score on the Arithmetic Reasoning test in 1981
Word | score on the Word Knowledge Test in 1981
Parag | score on the Paragraph Comprehension test in 1981
Numer | score on the Numerical Operations test in 1981

**Self-Esteem test 81 and 87**

We have two sets of self-esteem test, one in 1981 and the other in 1987. Each set has same 10 questions. 
They are labeled as `Esteem81` and `Esteem87` respectively followed by the question number.
For example, `Esteem81_1` is Esteem question 1 in 81.

The following 10 questions are answered as 1: strongly agree, 2: agree, 3: disagree, 4: strongly disagree

* Esteem 1: “I am a person of worth”
* Esteem 2: “I have a number of good qualities”
* Esteem 3: “I am inclined to feel like a failure”
* Esteem 4: “I do things as well as others”
* Esteem 5: “I do not have much to be proud of”
* Esteem 6: “I take a positive attitude towards myself and others”
* Esteem 7: “I am satisfied with myself”
* Esteem 8: “I wish I could have more respect for myself”
* Esteem 9: “I feel useless at times”
* Esteem 10: “I think I am no good at all”

## Data preparation

Load the data. Do a quick EDA to get familiar with the data set. Pay attention to the unit of each variable. Are there any missing values?

Here I'll read in the data and display some rows to make sure I've got what I'm expecting.
```{r}
esteem_data <- fread('data/NLSY79.csv') # read in the csv
head(esteem_data, 5) # show the first 5 rows to get a sense of what we have
```

Now, let's evaluate the structure, fields, missing values, etc. Here I'll use skimr to get a good sense as to what the data looks like, how it's distributed, and see if there are any missing values. I've also included a small line of code just reporting missing values (none). 
```{r}
skimr::skim(esteem_data) # get some quick information about the data
# although skimr will display any missing values, for convenience I'll use the statement below
sprintf('There are %i missing values in the dataset', sum(is.na(esteem_data))) # report the number of missing values as a whole
```


## Self esteem evaluation

Let concentrate on Esteem scores evaluated in 87. 

1. Reverse Esteem 1, 2, 4, 6, and 7 so that a higher score corresponds to higher self-esteem. (Hint: if we store the esteem data in `data.esteem`, then `data.esteem[,  c(1, 2, 4, 6, 7)]  <- 5 - data.esteem[,  c(1, 2, 4, 6, 7)]` to reverse the score.)

Here I'll reverse the score on a selection of items such that higher value indicates higher self-esteem, and save the result into a new variable. This will make further analysis easier. 
```{r}
esteem_data_adjusted <- esteem_data
esteem_data_adjusted[,  c(37, 38, 40, 42, 43)] <- 5 - esteem_data_adjusted[,  c(37, 38, 40, 42, 43)] # reverse score such that higher value indicates higher level of self esteem construct for these items
```

2. Write a brief summary with necessary plots about the 10 esteem measurements.

In this section I'll evaluate some statistics that describe each of the 10 items, and then generate some histogram plots. First I'll start by gathering some descriptive stats.
```{r}
# let's report some basic stats about each item
x <- esteem_data_adjusted[, 37:46]
sapply(x, function(x) c( "Mean"= mean(x),
                         "SD" = sd(x), 
                         "Median" = median(x),
                         "Minimum" = min(x),
                         "Maximun" = max(x)
                    )
)
```

Now, I'll generate some histograms so we can have a visual representation of what each item looks like.
```{r, fig.width=10, fig.height=8}
# Plot each item's histogram
ggplot(gather(esteem_data_adjusted[, 37:46]), aes(value)) + 
  geom_histogram(binwidth = 1, fill = 'seagreen4', alpha = 0.8) + 
  facet_wrap(~key, scales = 'free_x') + 
  ggtitle("Self-Esteem Questionnaire Itemwise Score Distributions") +
  xlab("Score") +
  ylab('Count') 

```

As we can see, items generally have a mean somewhere between 3 and 4, with a standard deviation between about half a point to around a quarter of a point. All items have distributions slightly or very skewed in favor of higher scores. Taken together, this information suggests that moderate-to-high self-esteem is more typical, while extremely low elf-esteem is less frequent. 

3. Do esteem scores all positively correlated? Report the pairwise correlation table and write a brief summary.

To do this I will calculate a correlation matrix for the 10 items. 
```{r}
esteem_pairwise_corr <- cor(esteem_data_adjusted[, 37:46])  # calculate pairwise correlation matrix given the esteem measures
esteem_pairwise_corr # let's see the matrix
```

Now, I'll plot a heatmap of the correlation matrix to get a quick visual sense of how correlated these items are. 
```{r, fig.width=10, fig.height=8, warning=FALSE, fig.keep='all'}
ggplot(data = melt(esteem_pairwise_corr), aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() +
  ggtitle('Self-Esteem_87 Items Correlation Matrix') + 
  xlab('Item') +
  ylab('Item') +
  labs(fill = "Correlation") 
```
As we can see, while the degree to which items are correlated varies, there are no negative correlations, indicating that the items in this questionnaire are indeed positively correlated to each other. 

4. PCA on 10 esteem measurements. (centered but no scaling)

    a) Report the PC1 and PC2 loadings. Are they unit vectors? Are they orthogonal? 
  
    b) Are there good interpretations for PC1 and PC2? (If loadings are all negative, take the positive loadings for the ease of interpretation)
  
    c) How is the PC1 score obtained for each subject? Write down the formula.
    
    d) Are PC1 scores and PC2 scores in the data uncorrelated? 
    
    e) Plot PVE (Proportion of Variance Explained) and summarize the plot. 
  
    f) Also plot CPVE (Cumulative Proportion of Variance Explained). What proportion of the variance in the data is explained by the first two principal components?
  
    g) PC’s provide us with a low dimensional view of the self-esteem scores. Use a biplot with the first two PC's to display the data.  Give an interpretation of PC1 and PC2 from the plot. (try `ggbiplot` if you could, much prettier!)

a) I've displayed the PC loadings below. PCs are indeed unit vectors that are orthogonal to each other. This is necessarily the case. 
```{r}
# first, for convenience, create a new table including only the items of interest
esteem_87_measure <- esteem_data_adjusted[, 37:46]
# now, we apply PCA without scaling 
esteem_87_pca <- prcomp(esteem_87_measure, scale. = FALSE)
# display the loadings
esteem_87_pca$rotation
```

b) It seems PC1 can be interpreted as overall self-esteem, and serves as a weighted sum of all the items. PC2 is interesting, separating the last 3 items in the questionnaire from the rest. Based on the content of those three last questions, it can be intuited that PC2 is identifying two distinct domains of self-esteem, self-regard vs personal utility. Here, higher scores on PC2 would indicate a higher sense of personal utility.  

c) The formula, given the loadings, would be as such: 
$$
Score = 0.235 \times Esteem87_1 + 0.244 \times Esteem87_2 + 0.279 \times Esteem87_3 + 0.261 \times Esteem87_4 + \\ 0.312 \times Esteem87_5 + 0.313 \times Esteem87_6 + 0.299 \times Esteem87_7 + 0.393 \times Esteem87_8 + \\ 0.398 \times Esteem87_9 + 0.376 \times Esteem87_10
$$

d) While necessarily PCs are uncorrelated, I've gone ahead and generated a correlation matrix below to demonstrate. 
```{r}
# assess correlation between PC1 and PC2 scores
cor(esteem_87_pca$x[,c(1,2)])
```

e) As we can see below, the proportion of overall variance explained by PC1 is about 46%, and as expected each subsequent PC explains less of the overall variance than the previous. 
```{r, fig.width=10, fig.height=8}
# for convenience, extract the PVE and CPVE
esteem_PVE <- data.frame(summary(esteem_87_pca)$importance[2, ])
esteem_CPVE <- data.frame(summary(esteem_87_pca)$importance[3, ])

# get a PVE plot for our principal components
ggplot(data=esteem_PVE, aes(x = factor(1:dim(esteem_PVE)[1]), y = esteem_PVE[,1])) + 
  geom_bar(stat="identity", fill = 'seagreen4', alpha = 0.8) +
  xlab("Principal Componnents") +
  ylab("Proportion of Variance Explained") +
  ggtitle("PVE Scree Plot") +
  scale_x_discrete(breaks = seq(1,10,by=1))
```

f) As we can see in the plot below, the first two princial components combined explain roughly 60% of the variance. 
```{r, fig.width=10, fig.height=8}
# get a cumulative PVE plot for our principal components
ggplot(data=esteem_CPVE, aes(x = factor(1:dim(esteem_CPVE)[1]), y = esteem_CPVE[,1])) + 
  geom_bar(stat="identity", fill = 'steelblue1', alpha = 0.8) +
  xlab("Principal Componnents") +
  ylab("Cummulative Proportion of Variance Explained") +
  ggtitle("Cummulative PVE Plot") +
  scale_x_discrete(breaks = seq(1,10,by=1))
```

g) The biplot below shows two divergent "streams", corresponding to variance along PC2. This is consistent with our earlier interpretation of PC2 as picking out two separate domains of self-esteem (with Items 8-10 inclusive being indicative of a putative "personal utility" domain). 
```{r, fig.width=10, fig.height=8}
# create a biplot for our first two PCs - here I use the AMR package function ggplot_pca()
ggplot_pca(x=esteem_87_pca, arrows_textsize = 3, arrows_colour = "red4", points_size = 1) + xlab("PC1") + ylab("PC2") + ggtitle("Biplot of first two Principal Components")
```


5. Apply k-means to cluster subjects on the original esteem scores

    a) Find a reasonable number of clusters using within sum of squared with elbow rules.
    
    b) Can you summarize common features within each cluster?
    
    c) Can you visualize the clusters with somewhat clear boundaries? You may try different pairs of variables and different PC pairs of the esteem scores.

a) Based on the Within Sum of Squares plot below, I have decided to use 3 clusters, as this seems to correspond with a reasonable "elbow". 
```{r, fig.width=10, fig.height=4}
# determine optimal number of clusters
fviz_nbclust(esteem_87_measure, kmeans, method = "wss")
num_clusters = 3 # based on elbow method 
esteem_87_clust <- kmeans(esteem_87_measure, centers = num_clusters) # cluster into 3 groups
```

b) To get a quick visual sense of some cluster characteristics, I've calculated the mean score along each item for each cluster, and plotted them. As we can see in the blow below some cluster characteristics are as follows:
- Cluster 1: This cluster seems to have overall lower scores on self-esteem items. 
- Cluster 2: This cluster seems to have overall higher scores on self-esteem items.
- Cluster 3: This cluster seems to have higher scores on earlier items, and lower scores on later items. If we combine this with our interpretation of PC2 earlier, we can surmise that this cluster seems to have higher self-regard, but lower perceived personal utility. 
```{r, fig.width=10, fig.height=5, warning=FALSE}
# to evaluate, let's add cluster as a variable
esteem_raw_clust <- esteem_87_measure
esteem_raw_clust$cluster <- factor(esteem_87_clust$cluster)
# get at some cluster differences
cluster_avg_scores <- esteem_raw_clust %>%
  group_by(cluster) %>%
  summarise_at(vars(c(1:10)), mean) %>%
  data.frame()
ggplot(data = melt(cluster_avg_scores), aes(x = variable, y = value, group = cluster, col = cluster)) + 
  geom_line() + 
  geom_point() +
  ggtitle("Common Cluster Features") +
  ylab("Cluster Avg. Score") +
  xlab("Item")
```

c) After a trying different combinations, I felt that plotting the clusters against their PCs gave the clearest grouping, visualized below. 
```{r, fig.width=10, fig.height=5}
# Visualize clusters
# Plotting by the first two PCs
ggplot(data = data.frame(esteem_87_pca$x), aes(x=PC1, y=PC2, colour = esteem_raw_clust$cluster)) + 
  geom_point() +
  ggtitle("Clusters by PC1 & PC2 Scores")
```

6. We now try to find out what factors are related to self-esteem? PC1 of all the Esteem scores is a good variable to summarize one's esteem scores. We take PC1 as our response variable. 

    a) Prepare possible factors/variables:

      - Personal information: gender, education (05, problematic), log(income) in 87, job type in 87, Body mass index as a measure of health (The BMI is defined as the body mass divided by the square of the body height, and is universally expressed in units of kg/m²). Since BMI is measured in 05, this will not be a good practice to be inclueded as possible variables. 
          
      - Household environment: Imagazine, Inewspaper, Ilibrary, MotherEd, FatherEd, FamilyIncome78. Do set indicators `Imagazine`, `Inewspaper` and `Ilibrary` as factors. 
    
      - Use PC1 of SVABS as level of intelligence
        
    b)   Run a few regression models between PC1 of all the esteem scores and suitable variables listed in a). Find a final best model with your own criterion. 

      - How did you land this model? Run a model diagnosis to see if the linear model assumptions are reasonably met. 
        
      - Write a summary of your findings. In particular, explain what and how the variables in the model affect one's self-esteem. 
        
a) Here I will generate a new data.frame that contains my variables of interest. I am excluding measures collected in 2005, as this well after when the outcome variable we're intereted in was measured. I am taking the log of income for both personal income and family income, as this better gets at relative differences in both, and it is well established in psychology literature that in terms of affecting behavior and cognition, it is not absolute, but relative, differences in wealth that underlie effects (https://journals.sagepub.com/doi/abs/10.1177/0956797610362671). 
```{r}
# ok, we'll be building a data.frame that contains potential explanatory factors. initialize from our response variable
selfEsteem_model <- data.frame(esteem_87_pca$x[,1])
names(selfEsteem_model) <- "self_esteem"
# add personal information factors
selfEsteem_model$gender <- esteem_data$Gender 
selfEsteem_model$log_income <- log(esteem_data$Income87 + (abs(min(esteem_data$Income87)) + 0.0001)) # log of income, adding a small amount to mitigate NaNs
# add household environment factors
selfEsteem_model$magazine <- factor(esteem_data$Imagazine)
selfEsteem_model$newspaper <- factor(esteem_data$Inewspaper)
selfEsteem_model$library <- factor(esteem_data$Ilibrary)
selfEsteem_model$motherEd <- esteem_data$MotherEd
selfEsteem_model$fatherEd <- esteem_data$FatherEd
selfEsteem_model$famIncome <- log(esteem_data$FamilyIncome78 + (abs(min(esteem_data$FamilyIncome78)) + 0.0001))  # log of family income, adding a small amount to mitigate NaNs
# intelligence proxy (PC1 of SVAB)
svab_pca <- prcomp(esteem_data[,16:25], scale. = TRUE) # apply PCA to SVAB
selfEsteem_model$intelligence <- svab_pca$x[,1]
```

b) Here I will run several regression models to try to predict self esteem. I will generate and compare 3 models:

- A family Socio-Economic-Status model that uses maternal and paternal education, along with family income, as predictors. 
- A household media consumption model that uses consumption of magazines, newspapers, and library attendance as predictors. 
- A model based on income and SVAB-measured "intelligence" as predictors. 

I have chosen these because SES during development, media consumption habits, and income & intelligence have all been linked to self-esteem in prior literature. I have first evaluated if there are gender differences in self-esteem overall via an Analysis of Variance, and found that this was indeed the case, with males having overall higher levels of self-esteem. Because of this, I chose to include gender as a variable in my linear models, at least initially. I have also chosen to apply B&H False-Discovery-Rate correction to my analyses, since we are starting to make more comparisons here. 

My first step was to check for gender differences. We can see there are indeed gender differences, so I will include it as a variable in my models. 
```{r}
# are there gender differences in self esteem as measured by the questionnaire?
gender_diff <- aov(self_esteem ~ gender, data=selfEsteem_model)
summary(gender_diff)
```

Next, I will run the family SES model. 
```{r}
# this model will test the hypothesis that family SES during development mediates eventual self-esteem
family_SES_model <- lm(self_esteem ~ motherEd + fatherEd + famIncome + gender, data = selfEsteem_model)
summary(family_SES_model)
FamSES_fdr_p <- p.adjust(p = summary(family_SES_model)$coefficients[,4], method = "BH", n = length(summary(family_SES_model)$coefficients[,4]))
FamSES_fdr_p
```

Now, the Household Media Consumption model.
```{r}
# this model will test the hypothesis that household media habits mediate self esteem
household_media_model <- lm(self_esteem ~ magazine + newspaper + library + gender, data = selfEsteem_model)
summary(household_media_model)
HouseMEdia_fdr_p <- p.adjust(p = summary(household_media_model)$coefficients[,4], method = "BH", n = length(summary(household_media_model)$coefficients[,4]))
HouseMEdia_fdr_p
```

Finally, I will run the Intelligence and Income model. 
```{r} 
# this model will test the hypothesis that intelligence and income (two highly related variables in the literature) mediate self-esteem
intelligence_income_gender_model <- lm(self_esteem ~ intelligence + log_income + gender, data = selfEsteem_model)
summary(intelligence_income_gender_model)
IntelIncomeGender_fdr_p <- p.adjust(p = summary(intelligence_income_gender_model)$coefficients[,4], method = "BH", n = length(summary(intelligence_income_gender_model)$coefficients[,4]))
IntelIncomeGender_fdr_p
```

Comparing all 3 models, I have chosen the intelligence + income model based on having the highest R^2, and lowest residual standard error. However, we can see that gender is not very predictive in this model, so let's do an F-Test to see if we can remove it. 
```{r}
# let's see if we can reasonably remove the gender variable by conducting an F test
intelligence_income_model <- lm(self_esteem ~ intelligence + log_income, data = selfEsteem_model) # reduced model
anova(intelligence_income_model, intelligence_income_gender_model) # run test 
```

As we can see, p = 0.74. We cannot reject the null, so we can reasonably remove gender as a variable. This is likely due to most of the gender information being captured by the large income gender disparity at the time of data acquisition. Now, let's display the new (reduced) model, and FDR-corrected p-values. 
```{r}
# since we cannot reject the null, we will stick with the model that doesn't include gender
summary(intelligence_income_model)
IntelIncome_fdr_p <- p.adjust(p = summary(intelligence_income_model)$coefficients[,4], method = "BH", n = length(summary(intelligence_income_model)$coefficients[,4]))
IntelIncome_fdr_p
```

Just for ease of comparison, I've laid out the three models here:
```{r results='asis'}
stargazer(family_SES_model, household_media_model, intelligence_income_model,  type = 'html', 
                     keep.stat = c("n", "rsq", "sigma2", "ser"))
```



Now I will run some diagnostics on my income + intelligence model. We can see in the plots below that the asusmptions of linearity, normality, and homoscedasticity are reasonably (albeit not perfectly) met. 
```{r, fig.width=10, fig.height=6}
plot(intelligence_income_model, 1) # residual, check linearity
plot(intelligence_income_model, 2) # qq, check normality
plot(intelligence_income_model, 3) # scale-location, check homoscedasticity
```



# Case study 2: Breast cancer sub-type


[The Cancer Genome Atlas (TCGA)](https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga), a landmark cancer genomics program by National Cancer Institute (NCI), molecularly characterized over 20,000 primary cancer and matched normal samples spanning 33 cancer types. The genome data is open to public from the [Genomic Data Commons Data Portal (GDC)](https://portal.gdc.cancer.gov/).
 
In this study, we focus on 4 sub-types of breast cancer (BRCA): basal-like (basal), Luminal A-like (lumA), Luminal B-like (lumB), HER2-enriched. The sub-type is based on PAM50, a clinical-grade luminal-basal classifier. 

* Luminal A cancers are low-grade, tend to grow slowly and have the best prognosis.
* Luminal B cancers generally grow slightly faster than luminal A cancers and their prognosis is slightly worse.
* HER2-enriched cancers tend to grow faster than luminal cancers and can have a worse prognosis, but they are often successfully treated with targeted therapies aimed at the HER2 protein. 
* Basal-like breast cancers or triple negative breast cancers do not have the three receptors that the other sub-types have so have fewer treatment options.

We will try to use mRNA expression data alone without the labels to classify 4 sub-types. Classification without labels or prediction without outcomes is called unsupervised learning. We will use K-means and spectrum clustering to cluster the mRNA data and see whether the sub-type can be separated through mRNA data.

We first read the data using `data.table::fread()` which is a faster way to read in big data than `read.csv()`. 

```{r case2 load}
brca <- fread("data/brca_subtype.csv") #note - csv not pushed to github repo due to size

# get the sub-type information
brca_subtype <- brca$BRCA_Subtype_PAM50
brca_notype <- brca[,-1]

#visualize dataframe
dim(brca_notype)  #rows, columns
names(brca_notype)[1:20]  # see a few variable names
summary(brca_notype[, 1:3])  # only to peek the first 3 variables - lots of variability!
sum(is.na(brca_notype))   # quick way to check missing values. use with caution - missing values may not be coded as "na"
```

1. Summary and transformation
    
a) How many patients are there in each sub-type? 
    
``` {r case2 1.a}
table(brca_subtype)
```
    There are `r table(brca_subtype)[names(table(brca_subtype)) == "Basal"]` patients with Basal-like cancer, `r table(brca_subtype)[names(table(brca_subtype)) == "Her2"]` patients with HER2-enriched cancer, `r table(brca_subtype)[names(table(brca_subtype)) == "LumA"]` patients with Luminal A cancer and `r table(brca_subtype)[names(table(brca_subtype)) == "LumB"]`patients with Luminal B cancer.
    
b) Randomly pick 5 genes and plot the histogram by each sub-type.

``` {r case2 1.b}
#count number of genes (each column)
num_gene <- ncol(brca_notype)
# set seed for reproducibility and randomly select 5 numbers
set.seed(5)
sample_idx <- sample(num_gene, 5) 

# plot frequency of each gene
brca %>% 
  select(all_of(sample_idx), BRCA_Subtype_PAM50) %>%
  pivot_longer(cols = 1:5) %>% 
  ggplot(aes(x = value)) +
  geom_histogram(aes(fill = name)) +
  facet_grid(vars(BRCA_Subtype_PAM50), vars(name)) +
  ggtitle("5 Gene Frequencies by Cancer Subtype")

```

c) Remove gene with zero count and no variability. Then apply logarithmic transform.

```{r case2 1.c}
# remove genes with 0 counts
sel_cols <- which(colSums(abs(brca_notype)) != 0) #list columns that aren't entirely 0's
brca_sub <- brca_notype[, sel_cols, with=F] #keep only those columns in new df w/o type
dim(brca_sub) #now has ~300 fewer genes

#remove genes with 0 variance
zero_var <- names(which(apply(brca_sub, 2, var) != 0))
brca_sub <- brca_sub[, zero_var, with=F]
dim(brca_sub) #no genes removed

# log transform, adding 0.000000001 so that you're never trying to take log(0)
brca_log <- log2(as.matrix(brca_sub+1e-10))
```

2. Apply kmeans on the transformed dataset with 4 centers and output the discrepancy table between the real sub-type `brca_subtype` and the cluster labels.

``` {r case2 kmeans, eval=F}
set.seed(0)
#running kmeans with 4 centers, 10 starts
brca_sub_kmeans <- kmeans(x = brca_log, centers=4, nstart = 10)

#saving output because it seems like a good idea
saveRDS(brca_sub_kmeans, "output/brca_kmeans.RDS") #note - csv not pushed to git due to size
```

``` {r case2 kmeans2}
# read in kmeans so we don't have to keep rerunning
brca_sub_kmeans <- readRDS("output/brca_kmeans.RDS")

# create a discrepancy table to see how our clusters line up with known subtypes
table(brca_subtype, brca_sub_kmeans$cluster)
```

Clustering based on log-transformed values doesn't seem to do a great job revealing known subtypes.

3. Spectrum clustering: to scale or not to scale?

a) Apply PCA on the centered and scaled dataset. How many PCs should we use and why? You are encouraged to use `irlba::irlba()`.

``` {r case2 3.a, eval=F}
#running PCA with 3 methods to test - using log-transformed data due to right skew

## method 1: prcomp()
pca_brca_test <- prcomp(brca_log, center= TRUE, scale=TRUE) 
## save first 20
pca_brca_test$rotation <- pca_brca_test$rotation[, 1:20]   
pca_brca_test$x <- pca_brca_test$x[, 1:20]
saveRDS(pca_brca_test, "output/brca_pca_full.RDS")
```
```{r}
## read in kmeans so we don't have to keep rerunning
pca_brca_test <- readRDS("output/brca_pca_full.RDS") 

## method 2: prcomp_irlba()
pca_brca <- prcomp_irlba(brca_log, n=20, center = T, scale. = T)

## method 3: irlba()
### center and scale the data
brca_sub_scaled_centered <- scale(as.matrix(brca_log), center = T, scale = T)
### calculate 20 leading components using SVD
svd_ret <- irlba::irlba(brca_sub_scaled_centered, nv = 20)
### Approximate the PVE
svd_var <- svd_ret$d^2/(nrow(brca_sub_scaled_centered)-1)
pve_apx <- svd_var/num_gene
### get pc score
pc_score <- brca_sub_scaled_centered %*% svd_ret$v[, 1:3]

#table to see differences in PVE of PCs calculated with different methods
pve_table <- data.table(pr.comp = summary(pca_brca)$importance[2, ],
                pr.comp_irlba = summary(pca_brca_test)$importance[2,1:20],
                irlba = pve_apx) %>%
              as.data.frame()
pve_df <- data.frame(t(pve_table[1:3]))
names(pve_df) <- gsub("X", "PC", names(pve_df))
print(pve_df)

#plot PVEs to see how many PCs we should use
knitr::kable(summary(pca_brca)$importance)
plot(pca_brca)
summary(pca_brca)

#scree plot
plot(summary(pca_brca)$importance[2, ],
     ylab="PVE",
     xlab="Number of PC's",
     pch = 16, 
     main="Scree Plot of PVE for Scaled & Centered Data using SVD")
```

We should use 4 PCs because the 4th is where the variability explained drops, but not much variance is explained overall (as seen in the scree plot below).

```{r}
#scree plot of cumulative variance
plot(summary(pca_brca)$importance[3, ],
     ylab="Cumulative PVE",
     xlab="Number of PC's",
     pch = 16, 
     main="Scree Plot of Cumulative Variance for Scaled & Centered Data")
```
    
b) Plot PC1 vs PC2 of the centered and scaled data and PC1 vs PC2 of the centered but unscaled data side by side. Should we scale or not scale for clustering propose? Why?
    
``` {r}
#PCA on SCD with data unscaled
pca_brca_unscale <- prcomp_irlba(brca_log, n=20, center = T, scale = F)

#plot for scaled data
plot_scale <- as.data.frame(pca_brca$x) %>% 
  ggplot(aes(x=PC1, y=PC2)) +    # Try other PC's vs. PC1, any patterns?
  geom_point(color='blue')+
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0)  +
  ggtitle("Scaled") +
  theme_bw()

#plot for unscaled data
plot_unscale <- as.data.frame(pca_brca_unscale$x) %>% 
  ggplot(aes(x=PC1, y=PC2)) +    # Try other PC's vs. PC1, any patterns?
  geom_point(color='green')+
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  ggtitle("Unscaled") +
  theme_bw()

# put the plots together
comp2 <- ggpubr::ggarrange(plot_scale, plot_unscale, ncol = 2)
annotate_figure(p=comp2, top = text_grob("PC2 vs PC1 for Scaled and Unscaled Genetic Data", 
                               color = "black", face = "bold", size = 20))
```

```{r}
#looking at PVE for unscaled data to see how it compares
plot(summary(pca_brca_unscale)$importance[2, ],
     ylab="PVE",
     xlab="Number of PC's",
     pch = 16, 
     main="Scree Plot of PVE for Unscaled Data")

plot(summary(pca_brca_unscale)$importance[3, ], pch=16,
     ylab="Cumulative PVE",
     xlab="Number of PC's",
     main="Scree Plot of Cumulative PVE for Unscaled Genetic Data")
```

Given that the PCs calculated from scaled data capture more variance (i.e. PVE of PC1 in the scaled data is `r (summary(pca_brca)$importance[3])*100`% while the PVE of PC1 in the unscaled data is only `r summary(pca_brca_unscale)$importance[3]*100`%), using the scaled PCs would more accurately represent the original dataset. However, it is worth noting that there is a greater spread along the PC2 axis in the unscaled data, indicating that unscaled PC2 may have greater utility in distinguishing groups of subjects. 

4. Spectrum clustering: center but do not scale the data

a) Use the first 4 PCs of the centered and unscaled data and apply kmeans. Find a reasonable number of clusters using within sum of squared with the elbow rule.
    
``` {r}
#use elbow method - calculate within sum of squares for different ks (numbers of clusters) up to 15
set.seed(0)
fviz_nbclust(pca_brca_unscale$x[, 1:4], kmeans, k.max = 15, method = "wss") 
```

We should use 4 clusters because the 4th is where the total WSS drops, making it the most efficient cutoff point.

b) Choose an optimal cluster number and apply kmeans. Compare the real sub-type and the clustering label as follows: Plot scatter plot of PC1 vs PC2. Use point color to indicate the true cancer type and point shape to indicate the clustering label. Plot the kmeans centroids with black dots. Summarize how good is clustering results compared to the real sub-type.
    
```{r}
set.seed(0)
#cluster using 4 leading PCs and k=4
brca_spec <- kmeans(pca_brca_unscale$x[, 1:4], centers = 4, nstart=10)

#to plot, make a table that has the first two PCs, cancer subtype, and cluster
spec_table <- data.table(PC1 = pca_brca_unscale$x[,1], #PC1
                PC2 = pca_brca_unscale$x[,2], #PC2
                col = as.factor(brca_subtype), #subtype
                cl = as.factor(brca_spec$cluster))  #cluster

#save centroids in PC1 vs PC2 space
cent <- as.data.frame(brca_spec$centers) %>% select(1:4) %>% mutate(cl = as.factor(c(1:4)))

#plot
plot_spec <- spec_table %>%   ggplot() + 
  geom_point(aes(x = PC1, y = PC2, col = col, shape = cl)) +
  labs(color = "Cancer type", shape = "Cluster") +
  xlab("PC1") +
  ylab("PC2") +
  ggtitle("Leading Unscaled PCs") + 
  geom_point(data = cent, aes(x=PC1, y=PC2, shape=cl), size = 2, fill = "black")

plot_spec
```
    
Overall, the cluster analysis did not seem to do a very good job of distinguishing cancer sub-types, as indicated above. Subjects  with Basal cancer seem to be assigned to cluster 4 consistently, but overall the clusters do not reflect cancer sub-types well. 
    
c) Compare the clustering result from applying kmeans to the original data and the clustering result from applying kmeans to 4 PCs. Does PCA help in kmeans clustering? What might be the reasons if PCA helps?

```{r}
#kmeans on original log-transformed data: brca_sub_kmeans
#clustering on 4 unscaled PCs: brca_spec

# create a discrepancy table to see how our clusters line up with each other
table(brca_spec$cluster, brca_sub_kmeans$cluster)

#to plot, make a table that has the clusters & cancer subtypes
kmeans_table <- data.table(clust = brca_sub_kmeans$cluster, #cluster
                specclust = brca_spec$cluster, #spectrum cluster
                type = as.factor(brca_subtype)) #subtype

#plot
clust_bar <- kmeans_table %>% ggplot(aes(x=clust)) + 
  geom_bar(aes(fill=type), position="stack", stat="count") +
  xlab("Cluster")

spec_bar <- kmeans_table %>% ggplot(aes(x=specclust)) + 
  geom_bar(aes(fill=type), position="stack", stat="count") +
  xlab("Spectrum Cluster")

# put the plots together
comp3 <- ggpubr::ggarrange(clust_bar, spec_bar, ncol = 2, common.legend = T, legend = "bottom")
annotate_figure(p=comp3, top = text_grob("Clusters Assigned by Spectrum and Direct kmeans", 
                               color = "black", face = "bold", size = 20))

#other plot
heatmap(t(data.matrix(kmeans_table)))
```

There seems to be strong agreement between the clusters assigned by direct clustering and spectrum analysis, though neither clustering method seemed to distinguish cancer subtypes. It does not appear that PCA "helped" kmeans clustering of this dataset, assuming our goal was to recapitulate cancer subtypes. However, spectrum analyses can generally be helpful in terms of reducing the computational power necessary to conduct kmeans clustering. Furthermore by clustering on the first few PCs only, you are by definition emphasizing dimensions of the data that show the greatest variance and therefore will be most useful in distinguishing between clusters while ignoring low-variance components that may create ambiguity as to which cluster an observation belongs in. 
    
d) Now we have an x patient with breast cancer but with unknown sub-type. We have this patient's mRNA sequencing data. Project this x patient to the space of PC1 and PC2. (Hint: remember we remove some gene with no counts or no variablity, take log and centered) Plot this patient in the plot in iv) with a black dot. Calculate the Euclidean distance between this patient and each of centroid of the cluster. Can you tell which sub-type this patient might have? 
    
```{r}
x_patient <- fread("data/brca_x_patient.csv")

# remove genes with 0 counts
x_patient <- x_patient[, sel_cols, with=F]
#remove genes with 0 variance
x_patient <- x_patient[, zero_var, with=F]
# log transform, adding 0.000000001 so that you're never trying to take log(0)
pt_log <- log2(as.matrix(x_patient+1e-10))

# center the data on means of brca_log
pt_log_cent <- t(t(pt_log)/colMeans(brca_log))

#apply PC loadings for pt x
xpc1 <- sum(t(pt_log)*pca_brca_unscale$rotation[,1])
xpc2 <- sum(t(pt_log)*pca_brca_unscale$rotation[,2])
xpc3 <- sum(t(pt_log)*pca_brca_unscale$rotation[,3])
xpc4 <- sum(t(pt_log)*pca_brca_unscale$rotation[,4])
pt_x_table <- data.table(PC1 = xpc1,
                PC2 = xpc2,
                PC3 = xpc3,
                PC4 = xpc4)

#plot
x_plot <- plot_spec + 
  geom_point(data = pt_x_table, aes(x=PC1, y=PC2), size = 5, shape = "diamond", color = "darkblue") +
  ggtitle("Leading Unscaled PCs of Genetic Data with Supplementary Patient X")

x_plot

#calculate euclidean distance
euclidean <- function(a, b) {sqrt(sum((a - b)^2))} #define func to calculate distance
x_vec <- c(xpc1, xpc2, xpc3, xpc4)
x_dist_table <- data.table(dist1 = euclidean(x_vec, cent[,1]),
                           dist2 = euclidean(x_vec, cent[,2]),
                           dist3 = euclidean(x_vec, cent[,3]),
                           dist4 = euclidean(x_vec, cent[,4]))
#find closest center
colnames(x_dist_table)[max.col(-x_dist_table)]

#look at distribution of subtypes in that cluster
kmeans_table %>% subset(specclust==4, select=type) %>% summary()
```

Based on the distance from centers, Patient X would be assigned to cluster 4 in our analyses, indicating that they are most likely to have the Basal cancer subtype.


# Case study 3: Auto data set

This question utilizes the `Auto` dataset from ISLR. The original dataset contains 408 observations about cars. It is similar to the CARS dataset that we use in our lectures. To get the data, first install the package ISLR. The `Auto` dataset should be loaded automatically. We'll use this dataset to practice the methods learn so far. 
Original data source is here: https://archive.ics.uci.edu/ml/datasets/auto+mpg

Get familiar with this dataset first. Tip: you can use the command `?ISLR::Auto` to view a description of the dataset. 

## EDA
Explore the data, with particular focus on pairwise plots and summary statistics. Briefly summarize your findings and any peculiarities in the data.

```{r}
# exploring numerical descriptive summaries
auto_df <- Auto
# str(auto_df)
# summary(auto_df)

# changing origin variable to factor and rename factor levels
auto_df <- auto_df %>% 
  mutate(origin = as.factor(origin)) %>% 
  mutate(origin = recode_factor(origin, "1" = "American", 
                                "2" = "European",
                                "3" = "Japanese"))


# providing data summary now that the origin variable has been renamed
head(auto_df)
str(auto_df)
summary(auto_df)

# checking for NAs... There are none
# apply(is.na(Auto), 2, which)
```


```{r message=FALSE, warning=FALSE}

# providing more elaborate summary statistics of numerical variables by origin
agg <- auto_df %>% 
  group_by(origin) %>% 
  summarise(
    n_cars = length(origin),
    min_mpg = min(mpg), max_mpg = max(mpg), mean_mpg = mean(mpg), sd_mpg = sd(mpg),
    min_cyl = min(cylinders), max_cyl = max(cylinders), mean_cyl = mean(cylinders), sd_cyl = sd(cylinders),
    min_disp = min(displacement), max_disp = max(displacement), mean_disp = mean(displacement), sd_disp = sd(displacement),
    min_power = min(horsepower), max_power = max(horsepower), mean_power = mean(horsepower), sd_power = sd(horsepower),
    min_weight = min(weight), max_weight = max(weight), mean_weight = mean(weight), sd_weight = sd(weight),
    min_acc = min(acceleration), max_acc = max(acceleration), mean_acc = mean(acceleration), sd_acc = sd(acceleration),
    min_year = min(year), max_year = max(year), mean_year = mean(year), sd_year = sd(year),
    )

agg2 <- data.frame(t(agg[-1]))
colnames(agg2) <- c("American", "European", "Japanese")

agg2

```

Based on our numerical summary by origin, we note that: 

* The dataset reports on a much larger number of American cars (n = 245) compared to European (n = 68) or Japanese cars (n = 79
* Japanese cars appear to reach (both max and on average) the largest number of miles per gallon (`mpg`)
* American cars have more `cylinders` on average (mean = 6.3) compared to European (mean = 4.16) and Japanese cars (mean = 4.1)
* Engine `displacement` is much larger in American cars (mean = 248, max = 455) compared to European (mean = 110, max = 183) and Japanese cars (mean = 168, max = 103)
* American cars appear more powerful on average (mean = 119) compared to European (mean = 80.6) or Japanese cars (mean = 79.8)
* American cars appear heavier on average (mean = 3372 lbs) compared to European (mean = 2433 lbs) or Japanese cars (mean = 2221 lbs)
* Acceleration capacity is roughly similar in all three car origin types
* Information is reported for cars made between 1970-1982, so these are relatively old cars

Let's now view some exploratory plots: 

```{r, warning=FALSE, message=FALSE, fig.width=9, fig.height=8}

# create pairwise plots using the GGally package
auto_df %>% 
  select(-c(name)) %>%
  ggpairs(aes(color = origin))

```

  
Based on these pairwise plots, we note that: 

* MPG is strongly negatively correlated with the number of cylinders, engine displacement, horsepower, and weight. These relationships appear non-linear for displacement, horsepower and weight. 
* MPG shows moderately positive correlations with acceleration and year. These may be linear or non-linear. 
* American cars have on average a lower MPG, much larger number of cylinders, larger displacement, horsepower and weight. 
* Japanese cars are on average more recent than European and American cars. 
* Cylinders, displacement, horsepower, weight and acceleration are correlated between each other. 
* Year shows interesting trends: 
  + 5 cylinder cars appeared in the 80s
  + While displacement, horsepower and weight remained mostly similar over time in European and Japanese cars, these metrics have decreased over time in American cars (where cars in the 70s used to have much larger displacement, horse power and weight compared to the 80s).
* American cars are much more represented in the sample (larger N) compared to European and Japanese cars. 
  

## What effect does `time` have on `MPG`?

a) Start with a simple regression of `mpg` vs. `year` and report R's `summary` output. Is `year` a significant variable at the .05 level? State what effect `year` has on `mpg`, if any, according to this model. 

```{r}
# building a linear model with mpg ~ year
m1 <- summary(lm(data = auto_df, mpg ~ year))
m1

ggplot(auto_df, aes(x = year, y = mpg)) + 
  geom_point(color = "seagreen4") + 
  geom_smooth(method = "lm", color = "darkgray") + 
  labs(title = "Effect of year on MPG") +
  theme(plot.title = element_text(size = 20)) +
  theme_bw()
```

Year has a significant effect on MPG (p = `r format.pval(m1$coefficients[2,4], 3)`). Specifically, increasing year by 1 unit will lead to an increase of `r m1$coefficients[2,1]` in MPG. 

b) Add `horsepower` on top of the variable `year` to your linear model. Is `year` still a significant variable at the .05 level? Give a precise interpretation of the `year`'s effect found here. 

```{r}
# adding horsepower to the model
m2 <- summary(lm(data = auto_df, mpg ~ year + horsepower))
m2

```

Year still has a significant effect on MPG (p = `r format.pval(m2$coefficients[2,4], 3)`) when adding horsepower to the model. Specifically, in this model, increasing year by 1 unit while keeping horsepower constant will lead to an increase of `r m2$coefficients[2,1]` in MPG. On the other hand, increasing horsepower by 1 unit while keeping year constant will lead to a `r m2$coefficients[3,1]` decrease in MPG.  


c) The two 95% CI's for the coefficient of year differ among (i) and (ii). How would you explain the difference to a non-statistician?  

A 95% confidence interval for a given $\beta_i$ coefficient is defined as: 

$$\hat \beta_i \pm t^*_{.025} \text{se}(\hat \beta_i).$$
We also know that  $t^*_{.025}$ is approximately equal to $1.96$ for a data with a large number of observations such as here. Thus, let's now compute the confidence interval for models (i) and (ii): 

```{r}
# model (i) confidence interval
m1_conf <- 1.96 * m1$coefficients[2,2]

# model (ii) confidence interval
m2_conf <- 1.96 * m2$coefficients[2,2]

# creating table with lower and upper confidence intervals
conf_table <- rbind(c("lower" = m1$coefficients[2,1] - m1_conf, "upper" = m1$coefficients[2,1] + m1_conf),
                    c("lower" = m2$coefficients[2,1] - m2_conf, "upper" = m2$coefficients[2,1] + m2_conf))

rownames(conf_table) <- c("Model (i)","Model (ii)")
conf_table <- as.table(conf_table)
conf_table

```
  
We can see that the 95% confidence interval is indeed reduced for model (ii). In layman terms, the main difference between these two models is that an additional variable has been added in model (ii) (i.e., horsepower). Whenever a variable is included, it adds new information to help explain the data, and this information will therefore lead to a more accurate estimation of the linear fit. A confidence interval can be understood as the "level of uncertainty": the smaller it is, the more certain we are about our estimation of the regression line. Thus, it makes sense that when adding more variables to a model, the estimation becomes more accurate, and the confidence interval will automatically become narrower.  


d) Create a model with interaction by fitting `lm(mpg ~ year * horsepower)`. Is the interaction effect significant at .05 level? Explain the year effect (if any). 

```{r}
# adding horsepower to the model
m3 <- summary(lm(data = auto_df, mpg ~ year * horsepower))
m3

# plotting the interaction model
ggplot(auto_df, aes(x = horsepower, y = mpg, color = year)) + 
  geom_point() + 
  geom_smooth(method = "lm", color = "darkgray") + 
  labs(title = "Interaction effect of horsepower and year on MPG") +
  theme(plot.title = element_text(size = 20)) +
  theme_bw()

```

Yes, the interaction effect between year and horsepower is significant (p = `r format.pval(m3$coefficients[4,4], 3)`). In this model, year has both a main effect and an interaction effect:

* Main effect: increasing year by 1 unit will lead to a `r m3$coefficients[2,1]` increase in MPG
* Interaction effect: year appears to have a different effect on MPG depending on the horsepower. Based on the plot above, we can observe that cars produced in the late 70s - early 80s (lighter blue) have both higher horsepower and higher MPG, meaning that they are more powerful and efficient. On the other hand, cars from the early 70s (dark blue) have smaller horsepower and MPG, indicating that they are less powerful and less efficient. This observation makes sense, given that technological progress made over time has logically led to more powerful and more efficient cars.  

## Categorical predictors

Remember that the same variable can play different roles! Take a quick look at the variable `cylinders`, and try to use this variable in the following analyses wisely. We all agree that a larger number of cylinders will lower mpg. However, we can interpret `cylinders` as either a continuous (numeric) variable or a categorical variable.

a) Fit a model that treats `cylinders` as a continuous/numeric variable. Is `cylinders` significant at the 0.01 level? What effect does `cylinders` play in this model?

```{r}
# linear model with cylinder as a continuous variable
m4 <- summary(lm(data = auto_df, mpg ~ cylinders))
m4

# plotting the cylinders model
ggplot(auto_df, aes(x = cylinders, y = mpg)) + 
  geom_point(color="seagreen4") + 
  geom_smooth(method = "lm", color = "darkgray") + 
  labs(title = "Effect of cylinders (continuous) on MPG") +
  theme(plot.title = element_text(size = 20)) +
  theme_bw()

```

The number of cylinders has a significant effect on MPG (p = `r format.pval(m4$coefficients[2,4], 3)`) at the 0.01 level. Increasing the number of cylinders y 1 unit leads to a `r m4$coefficients[2,1]` decrease in MPG. 


b) Fit a model that treats `cylinders` as a categorical/factor. Is `cylinders` significant at the .01 level? What is the effect of `cylinders` in this model? Describe the `cylinders` effect over `mpg`. 

```{r}
# transform cylinders into categorical variable
auto_df <- auto_df %>% 
  mutate(cylinders_fact = as.factor(cylinders))

# checking transformation
# str(auto_df)
# levels(auto_df$cylinders_fact) # levels vary between 3-8. There are no 7 cylinder cars.

# running linear model with cylinders as factor
m5 <- summary(lm(data = auto_df, mpg ~ cylinders_fact))
m5

# running linear model with cylinders as factor
m6 <- summary(lm(data = auto_df, mpg ~ 0 + cylinders_fact))
m6

```

When treating the number of cylinders as a categorical variable, we can observe a differential impact of different numbers of cylinders on MPG. Specifically, it appears that having 5, 6 or 8 cylinders does not have a significant impact on MPG, whereas having 3 or 4 cylinders significantly impacts MPG at the 0.01 significance level. More specifically, cars with 3 cylinders are predicted to have a mean MPG of `r m5$coefficients[1,1]`, while cars with 4 cylinders will have a mean MPG of `r m6$coefficients[2,1]`, which is `r m5$coefficients[2,1]` MPG more. Of note, results differ when considering the `mpg ~ cylinders_fact` vs the `mpg ~ 0 + cylinders_fact`, as the first model reports the *difference* between cylinders groups compared to a reference group, i.e. 3 cylinders. In the second model, we estimate the mean in each cylinder level, and results show that all cylinder groups have a significant effect on MPG. 

```{r}
# plotting the cylinders (categorical) model
ggplot(auto_df, aes(x = cylinders_fact, y = mpg, color = cylinders_fact)) + 
  geom_boxplot() +
  geom_jitter(size = 1, alpha = 0.9) +
  geom_smooth(method = "lm", color = "darkgray") + 
  labs(title = "Effect of cylinders (categorical) on MPG") +
  theme(plot.title = element_text(size = 20)) +
  theme_bw()

```
  
The plot reveals that very few cars in this dataset have 3 or 5 cylinders compared to the other groups (4, 6 and 8 cylinders), which contain the vast majority of data points. Thus, let's rerun the same analysis after removing the 3 and 5 cylinder categories:

```{r}
# remove 3 and 5 cylinder levels
auto_df_sub <- auto_df %>% 
  filter(!grepl('3|5', cylinders_fact))

# running linear model with cylinders as factor
m7 <- summary(lm(data = auto_df_sub, mpg ~ cylinders_fact))
m7

# plotting the cylinders (categorical) model
ggplot(auto_df_sub, aes(x = cylinders_fact, y = mpg, color = cylinders_fact)) + 
  geom_boxplot() +
  geom_jitter(size = 1, alpha = 0.9) +
  geom_smooth(method = "lm", color = "darkgray") + 
  labs(title = "Effect of cylinders (categorical) on MPG") +
  theme(plot.title = element_text(size = 20)) +
  theme_bw()

```

  
Now, all three cylinder groups have a significant effect on MPG. As can be expected, cars with 4 cylinders have the best MPG, cars with 6 cylinders have an intermediate efficiency and cars with 8 cylinders have the lowest efficiency.  


c) What are the fundamental differences between treating `cylinders` as a continuous and categorical variable in your models? 

The fundamental difference is that when cylinders is treated as a continuous variable, this assumes a linear relationship between cylinders and MPG, without breaking it down by levels. By contrast, a model including cylinders as a continuous variable assesses the effect of each level (i.e., number of cylinders) on MPG separately and a linear relationship is not enforced on the data. In doing so, we can obtain a more specific estimate of the difference in MPG across cylinder groups.   

d) Can you test the null hypothesis: fit0: `mpg` is linear in `cylinders` vs. fit1: `mpg` relates to `cylinders` as a categorical variable at .01 level?  

To address this question, we need to perform an F-test using `anova()`, which determines the difference between a null hypothesis model (fit0: `mpg` is linear in `cylinders`) and an alternative hypothesis model (fit1: `mpg` relates to `cylinders` as a categorical variable). We are applying this test on a subset that only includes well represented cylinder groups (4,6,8).  

```{r}

# fit0: cylinders as a continouus variables
fit0 <- lm(data = auto_df_sub, mpg ~ cylinders)

# fit1: cylinders as a categorical variable
fit1 <- lm(data = auto_df_sub, mpg ~ cylinders_fact)

# testing the difference between fit0 and fit1
anova_cyl <- anova(fit0, fit1)
anova_cyl
pval <- anova_cyl$`Pr(>F)`[2]

```

  
Given that p = `r format.pval(pval, 3)`, we can reject the null hypothesis that `cylinders` as a continuous variable has a linear effect on MPG.    


## Results

Final modeling question: we want to explore the effects of each feature as best as possible. You may explore interactions, feature transformations, higher order terms, or other strategies within reason. The model(s) should be as parsimonious (simple) as possible unless the gain in accuracy is significant from your point of view.


  
### Basic (full) model

Let's start by building a full model including all features of potential interest to us in predicting `MPG`. We will directly include `cylinders` (with well-represented cylinder groups, i.e. 4,6 and 8 cylinders) as a categorical variable, as we determined that this variable is significantly better at explaining `MPG`. We will not consider the `name` variable in building our model, as this variable contains over 300 levels specifying a combination of make and model and thus will not be a very relevant variable. Finally, we will not include `acceleration` as per indications on Piazza.  

```{r}
# prepare dataframe containing only features of interest
auto_df2 <- select(auto_df_sub, -cylinders, -name, -acceleration)

# full model 
m_basic <- lm(data = auto_df2, mpg ~ .)
summary(m_basic)
tmp <- summary(m_basic)
```
 
Based on this initial full model, we can see that at the 0.05 significance level, all variables appear to have a significant effect on MPG the 8 cylinders level (p = `r format.pval(tmp$coefficients[9,4], 3)`). 

### Exploring interactions
```{r}

# full model with interactions
m_int <- lm(data = auto_df2, mpg ~ (.)^2)
summary(m_int)
tmp <- summary(m_int)
```
  
While the "goodness of fit" explains a large portion of the variance (R squared = `r tmp$r.squared`), the effect of most variables became non-significant and the interpretation of the effects has become very challenging. Thus, to optimize interpretability, we will restrict our investigation to models including main effects.  


### Exploring non-linear relationships 

Let's now consider the relationships of continuous independent variables with MPG. 


```{r, fig.width=8, fig.height=8}
# plotting displacement and MPG with both a linear and a quadratic fit
p1 <- ggplot(auto_df, aes(x = displacement, y = mpg)) + 
  geom_point(color="seagreen4") + 
  geom_smooth(method = "lm", color = "darkgray") + 
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1) +
  labs(title = "Relationship between displacement and MPG") +
  # theme(plot.title = element_text(size = 15)) +
  theme_bw()

  
# plotting horsepower and MPG with both a linear and a quadratic fit
p2 <- ggplot(auto_df, aes(x = horsepower, y = mpg)) + 
  geom_point(color="seagreen4") + 
  geom_smooth(method = "lm", color = "darkgray") + 
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1) +
  labs(title = "Relationship between horsepower and MPG") +
  # theme(plot.title = element_text(size = 15)) +
  theme_bw()
  
# plotting the weight and MPG with both a linear and a quadratic fit
p3 <- ggplot(auto_df, aes(x = weight, y = mpg)) + 
  geom_point(color="seagreen4") + 
  geom_smooth(method = "lm", color = "darkgray") + 
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1) +
  labs(title = "Relationship between weight and MPG") +
  # theme(plot.title = element_text(size = 15)) +
  theme_bw()

# plotting the year with a linear fit
p4 <- ggplot(auto_df, aes(x = year, y = mpg)) + 
  geom_point(color="seagreen4") + 
  geom_smooth(method = "lm", color = "darkgray") + 
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1) +
  labs(title = "Relationship between year and MPG") +
  # theme(plot.title = element_text(size = 15)) +
  theme_bw()
  
ggpubr::ggarrange(p1, p2, p3, p4, nrow = 2, ncol = 2)

```
  
Based on the plots above, it appears that while year shows a linear relationship with MPG, displacement, weight and horsepower may have non-linear relationships with MPG. Based on this observation, we will add quadratic terms to our model for displacement, weight and horsepower.  

```{r}

# creating quadratic terms
auto_df2$displacement2 <- auto_df2$displacement^2
auto_df2$horsepower2 <- auto_df2$horsepower^2
auto_df2$weight2 <- auto_df2$weight^2

# fitting a model that includes quadratic terms
m_quad <- lm(data = auto_df2, mpg ~ .)
summary(m_quad)

```

In this model, the linear terms of horsepower, weight and year are significant at the 0.05 level, as well as the quadratic term of weight and horsepower. Let's now run an F-test to determine whether this model is a significantly better fit:

```{r}
# testing the difference between a model with only linear terms and a model with quadratic terms
anova_quad <- anova(m_basic, m_quad)
anova_quad
pval <- anova_quad$`Pr(>F)`[2]

```

Given that p = `r format.pval(pval, 3)`, we can reject the null hypothesis that variables *only* have a linear effect on MPG. Adding quadratic terms (model 2) leads to a significantly better fit.  


### Improving model parsimony

As a number of variables in the model above were not significantly affecting MPG, we will try removing them from the model and assess the difference between the full and reduced models with an F-test. 

```{r}
# fitting a model that includes quadratic terms
m_quad_red <- lm(data = auto_df2, mpg ~ year + horsepower + horsepower2 + origin + weight + weight2)
summary(m_quad_red)

# testing the difference between the full quadratic model and the reduced model
anova_quad_red <- anova(m_quad_red, m_quad)
anova_quad_red
pval <- anova_quad_red$`Pr(>F)`[2]

```

Given that p = `r format.pval(pval, 3)`, we cannot reject the null hypothesis that the reduced model is sufficient in explaining the data. In other words, the full model including all quadratic terms does not significantly improve the fit. We will thus stick to this reduced model.    



a) Describe the final model. Include diagnostic plots with particular focus on the model residuals and diagnoses.

Here, we provide an overview of the main models we have considered in our selection procedure:  

```{r results='asis'}

stargazer(m_basic, m_quad, m_quad_red, type = output_format ) 

```

Model 3 appears to be both the best fit and the most parsimonious, which is why we consider this as our final model.  

Let's run some diagnostics on the final model to check that the model assumptions are met:  


```{r}

# generating diagnostic plots
par(mfrow = c(1,2), mar = c(5,2,4,2), mgp = c(3,0.5,0)) # plot(fit3) produces several plots
plot(m_quad_red, 1, pch = 16) # residual plot
abline(h = 0, col = "blue", lwd = 2)
plot(m_quad_red, 2) # qqplot

```

  
Overall, the model appears to respect the assumptions of linearity, homoscedasticity and normal distribution.  

b) Summarize the effects found.


The final model includes the following effects:  

$$\hat{y} = 2.79 +  0.125 \times horsepower  -0.016 \times weight + 0.781 \times year + 1.120 \times originEuropean + 1.5 \times originJapanese + 0.0004 \times horsepower^2 + 0 \times weight^2 $$

c) Predict the `mpg` of the following car: A red car built in the US in 1983 that is 180 inches long, has eight cylinders, displaces 350 cu. inches, weighs 4000 pounds, and has a horsepower of 260. Also give a 95% CI for your prediction.

```{r}

# creating a newcar dataframe with the same format as the auto dataframe
newcar <-  auto_df2[1, ] 
newcar["mpg"] <- "NA"
newcar["origin"] <- "American"
newcar["year"] <- 83
newcar["cylinders"] <- 8
newcar["displacement"] <- 350
newcar["displacement2"] <- 350^2
newcar["weight"] <- 4000
newcar["weight2"] <- 4000^2
newcar["horsepower"] <- 260
newcar["horsepower2"] <- 260^2

# prediction for the new car
predict(m_quad_red, newcar, interval = "confidence", se.fit = TRUE) 

```

  
The model predicts that for this car, the MPG will be 26, with a confidence interval of ± 3.5 (lower = 22.4, upper = 29.5). The prediction can be described as follows:  

$$\hat{y} = 2.79 +  0.125 \times 260  -0.016 \times 4000 + 0.781 \times 83 + 1.120 \times 0 + 1.5 \times + 0.0004 \times 260^2 + 0 \times 4000^2 = 26 MPG $$
