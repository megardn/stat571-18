---
title: "Modern Data Mining, HW 3"
author:
- Diego G. Davila
- Margaret Gardner
- Joelle Bagautdinova
date: 'Due: 11:59Pm,  2/27, 2022'
output:
  html_document:
    code_folding: show
    highlight: haddock
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '4'
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=8, fig.height=4)
options(scipen = 0, digits = 3)  # controls base R output
# check if you have ISLR package, if not, install it
if(!require('pacman')) {install.packages('pacman')}
pacman::p_load(ISLR, readxl, magrittr, dplyr, ggplot2, skimr, GGally, car, leaps) # add the packages needed
```


\pagebreak

# Overview

Multiple regression is one of the most popular methods used in statistics as well as in machine learning. We use linear models as a working model for its simplicity and interpretability. It is important that we use domain knowledge as much as we could to determine the form of the response as well as the function format for the factors. Then, when we have many possible features to be included in the working model it is inevitable that we need to choose a best possible model with a sensible criterion. `Cp`, `BIC` and regularizations such as LASSO are introduced. Be aware that if a model selection is done formally or informally, the inferences obtained with the final `lm()` fit may not be valid. Some adjustment will be needed. This last step is beyond the scope of this class. Check the current research line that Linda and collaborators are working on. 

This homework consists of two parts: the first one is an excercise (you will feel it being a toy example after the covid case study) to get familiar with model selection skills such as, `Cp` and `BIC`. The main job is a rather involved case study about devastating covid19 pandemic.  Please read through the case study first. It is time that group members work together to run a real project. This project is for sure a great one listed in your CV. 

For covid case study, the major time and effort would be needed in EDA portion.

## Objectives

- Model building process

- Methods
    - Model selection
        + All subsets
        + Forward/Backward
    - Regularization
        + LASSO (L1 penalty)
        + Ridge (L2 penalty)
        + Elastic net
- Understand the criteria 
    - `Cp`
    - Testing Errors
    - `BIC` 
    - `K fold Cross Validation`
    - `LASSO` 
- Packages
    - `lm()`, `Anova`
    - `regsubsets()`
    - `glmnet()` & `cv.glmnet()`

# Review materials

- Study lecture: Model selection
- Study lecture: Regularization
- Study lecture: Multiple regression

Review the code and concepts covered during lectures: multiple regression, model selection and penalized regression through elastic net. 

# Case study 1:  `ISLR::Auto` data

This will be the last part of the Auto data from ISLR. The original data contains 408 observations about cars. It has some similarity as the Cars data that we use in our lectures. To get the data, first install the package `ISLR`. The data set `Auto` should be loaded automatically. We use this case to go through methods learned so far. Final modelling question: We want to explore the effects of each feature as best as possible. 

1) Preparing variables:

``` {r clean.auto}
#loading data
auto <- ISLR::Auto
#seeing what's there
str(auto)
summary(auto)
sum(is.na(auto))

#recoding as factors, removing name
auto <- auto %>% mutate(cylinders = as.factor(cylinders),
         origin = as.factor(origin),
         origin = recode_factor(origin, "1" = "American", 
                                "2" = "European",
                                "3" = "Japanese")) %>%
        select(-name)
#skim data
skim(auto)

#look for correlations
auto %>% select_if(is.numeric) %>%
  ggpairs()

#visualize skewed var
hist(auto$displacement)
hist(auto$horsepower)
```

Overall, the data is fairly clean with no missing values, as seen above. Displacement and horsepower both have right skew, but not severe enough to warrant log-transforming a predictor. Cylinders was recoded as a factor based on the non-linear relationship observed in similar analyses of mpg conducted for Homework 2, Case Study 3.

MPG has strong negative correlations with displacement, weight, horsepower; each of these independent variables also has a strong, positive correlation with the other two factors. All correlations modeled are significant at the level of p < 0.001, indicating that each variable is interrelated. 

a) You may explore the possibility of variable transformations. We normally do not suggest to transform $x$ for the purpose of interpretation. You may consider to transform $y$ to either correct the violation of the linear model assumptions or if you feel a transformation of $y$ makes more sense from some theory. In this case we suggest you to look into `GPM=1/MPG`. Compare residual plots of MPG or GPM as responses and see which one might yield a more satisfactory patterns. In addition, can you provide some background knowledge to support the notion: it makes more sense to model `GPM`?  

Logically, GPM may be better because it more directly reflects fuel consumption. Additionally, mpg is difficult to compare between cars as a 1 mpg increase in a low-efficiency car (eg 12 to 13 mpg would be $\frac{1}{13} GPM - \frac{1}{12}GPM = -0.00641 GPM$) is not equivalent to a 1 mpg increase in a high efficiency car (eg 25 to 26 mpg would be $\frac{1}{26} GPM - \frac{1}{25}GPM = -0.00154 GPM$). To test this, we'll compare the residuals and Q-Q plots of MPG and GPM as fit by full models (i.e. including all possible predictors). Whichever better meets the assumptions needed of linear modeling (linearity, homoscedasticity, and normality) will be used as $y$ in building our model.

``` {r gpm}
#new gpm variable
auto_gpm <- auto %>% mutate(gpm = 1/mpg) %>%
  select(-mpg)

#full models for MPG and GPM
fit.mpg <- lm(mpg ~ ., auto)
fit.gpm <- lm(gpm ~ ., auto_gpm)

#plots mpg
par(mfrow=c(1,2), mar=c(5,2,4,2), mgp=c(3,0.5,0)) #set formatting
#plot residuals
plot(fit.mpg, 1, pch=16, main = "MPG")
abline(h=0, col="blue", lwd=2)
#plot qq
plot(fit.mpg, 2, main="MPG")

#plot gpm
par(mfrow=c(1,2), mar=c(5,2,4,2), mgp=c(3,0.5,0)) #set formatting
plot(fit.gpm, 1, pch=16, main = "GPM")
abline(h=0, col="blue", lwd=2)
plot(fit.gpm, 2, main="GPM")
```

As seen above, neither GPM or MPG have very well distributed residuals, so we'll try log-transforming mpg and taking the square root of gpm: 

``` {r mpg_log}
#log mpg
auto.log <- auto %>% mutate(log_mpg = log(mpg)) %>%
  select(-mpg)
fit.log_mpg <- lm(log_mpg ~ ., auto.log)

#plots
par(mfrow=c(1,2), mar=c(5,2,4,2), mgp=c(3,0.5,0)) #set formatting
plot(fit.log_mpg, 1, pch=16, main = "log MPG")
abline(h=0, col="blue", lwd=2)
plot(fit.log_mpg, 2, main = "log MPG")

#sqrt gpm
auto.sqrt.gpm <- auto_gpm %>% mutate(sqrt.gpm = sqrt(gpm)) %>%
  select(-gpm)
fit.sqrt_gpm <- lm(sqrt.gpm ~ ., auto.sqrt.gpm)

#plots
par(mfrow=c(1,2), mar=c(5,2,4,2), mgp=c(3,0.5,0)) #set formatting
plot(fit.sqrt_gpm, 1, pch=16, main="Square Root GPM")
abline(h=0, col="blue", lwd=2)
plot(fit.sqrt_gpm, 2, main="Square Root GPM")
```

While these transformed values look better in terms of residuals, interpretability is questionable. Therefore we'll proceed with GPM as our outcome variable, as it's homoscestasticity, linearity and normality are acceptable. 

``` {r gpm.corr}
#look for correlations
auto_gpm %>% select_if(is.numeric) %>%
  ggpairs()
```

Based on the correlation plot outputs, the predictor variables look like they have a more linear relationship with gpm than they had with mpg, further supporting the decision to use gpm.

b) You may also explore by adding interactions and higher order terms. The model(s) should be as *parsimonious* (simple) as possible, unless the gain in accuracy is significant from your point of view. 

From the correlation plots, there do not appear to be any higher-order relationships with gpm. However, based on similar analyses run in homework 2 Case Study 3, we'll try modelling displacement, weight and horsepower as quadratics.

``` {r gpm.nonlinear}
# plotting displacement and GPM with both a linear and a quadratic fit
p.d <- ggplot(auto_gpm, aes(x = displacement, y = gpm)) + 
  geom_point() + 
  geom_smooth(method = "lm", color = "darkgray") + 
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1) +
  labs(title = "Displacement vs GPM")

  
# plotting horsepower and GPM with both a linear and a quadratic fit
p.h <- ggplot(auto_gpm, aes(x = horsepower, y = gpm)) + 
  geom_point() + 
  geom_smooth(method = "lm", color = "darkgray") + 
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1) +
  labs(title = "Horsepower vs GPM")
  
# plotting the weight and GPM with both a linear and a quadratic fit
p.w <- ggplot(auto_gpm, aes(x = weight, y = gpm)) + 
  geom_point() + 
  geom_smooth(method = "lm", color = "darkgray") + 
  stat_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1) +
  labs(title = "Weight vs GPM") 

ggpubr::ggarrange(p.d, p.h, p.w, nrow = 1, ncol = 3)
```

Based on the data plotted above, we can confirm that a linear relationship will adequately describe GPM's relationships with displacement, weight and horsepower.

We will now test several models though for interaction effects, first based on a priori knowledge of our subject matter, then in a data-driven manner.

A Priori Interaction Testing:
First we'll look at origin, year, and cylinders, since each could feasibly have an in interaction with the car's efficiency at a given horespower, weight, acceleration or displacement; year and cylinders since this could affect the mechanics used in the car to achieve a given performance and origin since cars manufactured in different locations could be made differently/adhere to different efficiency standards.

```{r origin.interaction}
Anova(lm(gpm ~ horsepower + origin + horsepower*origin, auto_gpm))
Anova(lm(gpm ~ weight + origin + weight*origin, auto_gpm))
Anova(lm(gpm ~ acceleration + origin + acceleration*origin, auto_gpm))
Anova(lm(gpm ~ displacement + origin + displacement*origin, auto_gpm))
```

The above provides evidence that $origin*acceleration$ and $origin*weight$ may add value to the model.

```{r year.interaction}
summary(lm(gpm ~ horsepower + year + horsepower*year, auto_gpm))
summary(lm(gpm ~ weight + year + weight*year, auto_gpm))
summary(lm(gpm ~ acceleration + year + acceleration*year, auto_gpm))
summary(lm(gpm ~ displacement + year + displacement*year, auto_gpm))
```

The above provides evidence that $year*acceleration$, $year*horsepower$, and $year*weight$ may add value to the model.

```{r cylinders.interaction}
Anova(lm(gpm ~ horsepower + cylinders + horsepower*cylinders, auto_gpm))
Anova(lm(gpm ~ weight + cylinders + weight*cylinders, auto_gpm))
Anova(lm(gpm ~ acceleration + cylinders + acceleration*cylinders, auto_gpm))
Anova(lm(gpm ~ displacement + cylinders + displacement*cylinders, auto_gpm))
```

The above provides evidence that $cylinders*horsepower$ and $cylinders*acceleration$ may add value to the model.

Data-Driven Interaction Testing:
Next we'll explore all possible interactions at once. This technique will have a higher bar of significance since each beta must add value when controlling for other predictors. Because of this and it's data-driven nature, this will likely result more accurate and parsimonious predictors.

``` {r}
Anova(lm(gpm ~ (.)^2, auto_gpm))
```

These results suggest that that $acceleration*year$ and $horsepower*acceleration$ may add value to the model.

c) Use Mallow's $C_p$ or BIC to select the model.

First we'll run `regsubests` on a full linear model without interaction terms:

``` {r gpm.cp}
#cp for all possible d's
fit.gpm <- regsubsets(gpm ~., auto_gpm , nvmax=15, method="exhaustive")
sum_fit.gpm <- summary(fit.gpm)
sum_fit.gpm

#plot
plot(sum_fit.gpm$cp, xlab="Number of predictors", 
     ylab="Cp", col="red", pch=16)

#plot BIC for fun
plot(sum_fit.gpm$bic, xlab="Number of predictors", 
     ylab="BIC", col="red", type="p", pch=16)

fit.exh.var <- sum_fit.gpm$which #indicators if variable is included
#pull vars for gpm.final, d=3
predictors <- colnames(fit.exh.var)[fit.exh.var[3,]] 
predict_d3 <- predictors[-c(1)] #drop intercept
```
Based on the elbow rule looking both at CP and BIC, d=3 (`r predict_d3`) is sufficient to model gpm; this also avoids the difficulties of forcing the model to include all 5 levels of the categorical cylinders predictor.

Next we'll try to account for interaction effects, specifically $acceleration*year$ and $horsepower*acceleration$, which were identified in our data-driven interaction search. 

``` {r}
fit.gpm_int <- regsubsets(gpm ~ . + acceleration*year + horsepower*acceleration, auto_gpm , nvmax=15, method="exhaustive")
sum_fit.gpm_int <- summary(fit.gpm_int)
sum_fit.gpm_int

#plot
plot(sum_fit.gpm_int$cp, xlab="Number of predictors", 
     ylab="Cp", col="red", pch=16)

#plot BIC for fun
plot(sum_fit.gpm_int$bic, xlab="Number of predictors", 
     ylab="BIC", col="red", type="p", pch=16)
```

Based on the elbow rule, both the d=4 or d=5 models would be good choices for predicting gpm with this model.

``` {r}
fit.exh.var_int <- sum_fit.gpm_int$which #indicators if variable is included
#pull vars for interaction model d=4
predictors_int4 <- colnames(fit.exh.var_int)[fit.exh.var_int[4,]] 
predict_int4 <- predictors_int4[-c(1)]

#pull vars for interaction model d=5
predictors_int5 <- colnames(fit.exh.var_int)[fit.exh.var_int[5,]] 
predict_int5 <- predictors_int5[-c(1)]
```

However, both the model with 4 predictors (`r predict_int4`) and the model with 5 predictors (`r predict_int5`) include interaction terms but not main effects for those variables (i.e. acceleration:year but not main effects of year). Additionally, the d=5 model includes only one level of a categorical variable (cylinder). Rectifying both would require forcing variables into the model, reducing the degrees of freedom and ultimately changing the criteria upon which that model was selected in the first place, $C_p$. 

To resolve this, we'll compare our top contender models, the main-effects model of d=3 (modelling on `r predict_d3`) and the interaction-effects model of d=4 (modelling on `r predict_int4`) expanded to include the main effects of year and horsepower.

``` {r}
#main effects only
fit.main <- lm(gpm ~ horsepower + weight + year, auto_gpm)
Anova(fit.main)

#interaction effects model
fit.int <- lm(gpm ~ weight + acceleration + year + horsepower + acceleration*year + acceleration*horsepower, auto_gpm)
Anova(fit.int)

anova(fit.main, fit.int)
```

As anticipated, the larger model with interaction explains more of the variance, but the interaction of acceleration and year is no longer significant when year itself is forced in the model. We'll try running the model again removing this interaction term to see what happens.

``` {r}
fit.int2 <- lm(gpm ~ weight + acceleration + year + horsepower + acceleration*horsepower, auto_gpm)
Anova(fit.int2)
anova(fit.main, fit.int2)

summary(fit.main)
summary(fit.int2)
```

As seen above, both models have nearly identical explanatory power ($\sigma$ and Adjusted $R^2$); therefore we will chose the more parsimonious model without interaction effects for ease of interpretation. However, it is interesting to consider how as acceleration grows (cars take longer to accelerate from 0 to 60 mph) the inverse effect of horsepower on gpm decreases; possibly cars that take a long time to accelerate may be less efficient despite having a high horsepower (due to weight or other factors not considered here) and therefore will burn more fuel (higher gpm).

2) Describe the final model and its accuracy (Cp or BIC criterion). Include diagnostic plots with particular focus on the model residuals.

Modelling the residuals of final model:

```{r auto.diagnostic}
#plots
par(mfrow=c(1,2), mar=c(5,2,4,2), mgp=c(3,0.5,0)) #set formatting
plot(fit.main, 1, pch=16)
abline(h=0, col="blue", lwd=2)
plot(fit.main, 2)
```
  * Summarize the effects found.
  
Overall this model appears to meet the assumptions of normality, linearity and homoscedasticity based on the residuals plotted above. We can therefore conclude that as a car's horsepower or weight increase, gpm also increases, while gpm decreases with an increase in car year. However, each coefficient is fairly small, meaning that while the effects of these predictors are highly significant, they do not drastically change gpm. This can be described with the following equation:

$$GPM = 0.0997 +  8.10*10^{-5} \times horsepower +  1.24*10^{-5}\times weight -1.28*10^{-3} \times year$$

  * Predict the `mpg` of a car that is: built in 1983, in the US, red, 180 inches long, 8 cylinders, 350 displacement, 260 as horsepower, and weighs 4,000 pounds. Give a 95% prediction interval.

```{r newcar}
#create a new dataframe for the car being predicted
newcar <-  auto_gpm[1, ]  # Create a new row with same structure as in auto_gpm
#assign features

newcar[1:8] <- c("8", 350, 260, 4000, NA, 83, NA, NA)
newcar <- newcar %>% mutate(displacement = as.numeric(displacement),
                            horsepower = as.numeric(horsepower),
                            weight = as.numeric(weight),
                            acceleration = as.numeric(acceleration),
                            year = as.numeric(year),
                            gpm = as.numeric(gpm))
#set levels cylinders to match auto_gpm
newcar$cylinders <- factor(newcar$cylinders, levels = c("3", "4", "5", "6", "8")) 
newcar$origin <- factor(newcar$origin, levels = c("American", "European", "Japanese")) 

#run the prediction
car_predict <- predict(fit.main, newcar, interval = "prediction", level = 0.95) 
#convert gpm back to mpg for value and prediction interval
f_inv <- function(x){1/x} #define function to take inverse
mpg_predict <- sapply(car_predict, f_inv)
car_predict
mpg_predict
```

This new car will have a predicted fuel efficiency of `r mpg_predict[1]` mpg, with a 95% prediction interval of `r mpg_predict[3]` to `r mpg_predict[2]` mpg.

* Any suggestions as to how to improve the quality of the study?

This study could be improved by expanding the sample or assessing other factors that can affect fuel efficiency, such as type of transmission, mechanical resistance or aerodynamics.

# Case study 2: COVID

See covid_case_study.Rmd.

