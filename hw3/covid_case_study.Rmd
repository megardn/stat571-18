---
title: "COVID-19 Case Study, HW3 GROUP 18"
author:
- Diego G. Davila
- Margaret Gardner
- Joelle Bagautdinova
date: 'Due before midnight, Feb 27'
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
    number_sections: yes
  pdf_document:
    toc_depth: '4'
    number_sections: yes
urlcolor: blue
---

```{r Setup, include=FALSE, results='hide', warning=FALSE}
knitr::opts_chunk$set(echo = T, fig.width=8, fig.height=4)
options(scipen = 0, digits = 3)  # controls base R output

# Package setup
if(!require("pacman")) install.packages("pacman")

pacman::p_load(tidyverse, dplyr, ggplot2, ggthemes, data.table, lubridate,
               GGally, RColorBrewer, ggsci, plotROC, usmap,
               plotly, ggpubr, vistime, skimr, tidyr, glmnet, dgCMatrix, leaps, car)
```

# Background

The outbreak of the novel Corona virus disease 2019 (COVID-19) [was declared a public health emergency of international concern by the World Health Organization (WHO) on January 30, 2020](https://www.who.int/dg/speeches/detail/who-director-general-s-statement-on-ihr-emergency-committee-on-novel-coronavirus-(2019-ncov)). Upwards of [112 million cases have been confirmed worldwide, with nearly 2.5 million associated deaths](https://covid19.who.int/). Within the US alone, there have been [over 500,000 deaths and upwards of 28 million cases reported](https://covid.cdc.gov/covid-data-tracker/#trends_dailytrendscases). Governments around the world have implemented and suggested a number of policies to lessen the spread of the pandemic, including mask-wearing requirements, travel restrictions, business and school closures, and even stay-at-home orders. The global pandemic has impacted the lives of individuals in countless ways, and though many countries have begun vaccinating individuals, the long-term impact of the virus remains unclear.

The impact of COVID-19 on a given segment of the population appears to vary drastically based on the socioeconomic characteristics of the segment. In particular, differing rates of infection and fatalities have been reported among different [racial groups](https://www.cdc.gov/coronavirus/2019-ncov/covid-data/investigations-discovery/hospitalization-death-by-race-ethnicity.html), [age groups](https://www.cdc.gov/coronavirus/2019-ncov/covid-data/investigations-discovery/hospitalization-death-by-age.html), and [socioeconomic groups](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7221360/). One of the most important metrics for determining the impact of the pandemic is the death rate, which is the proportion of people within the total population that die due to the the disease. 

We assemble this dataset for our research with the goal to investigate the effectiveness of lockdown on flattening the COVID curve. We provide a portion of the cleaned dataset for this case study. 

There are two main goals for this case study. 

1. We show the dynamic evolvement of COVID cases and COVID-related death at state level.
2. We try to figure out what county-level demographic and policy interventions are associated with mortality rate in the US. We try to construct models to find possible factors related to county-level COVID-19 mortality rates.
3. This is a rather complex project. With our team's help we have made your job easier. 
4. Hide all unnecessary lengthy R-output. Keep your write up neat, readable. 

  
**Remark1:** The data and the statistics reported here were collected before February of 2021. 

**Remark 2:** A group of RAs spent tremendous amount of time working together to assemble the data. It requires data wrangling skills. 

**Remark 3:** Please keep track with the most updated version of this write-up.


# Data Summary

The data comes from several different sources: 

1. [County-level infection and fatality data](https://github.com/nytimes/covid-19-data) - This dataset gives daily cumulative numbers on infection and fatality for each county. 
    * [NYC data](https://github.com/nychealth/coronavirus-data)
2. [County-level socioeconomic data](https://www.ers.usda.gov/data-products/atlas-of-rural-and-small-town-america/download-the-data/) - The following are the four relevant datasets from this site.   
    i. Income - Poverty level and household income. 
    ii. Jobs - Employment type, rate, and change.
    iii. People - Population size, density, education level, race, age, household size, and migration rates.
    iv. County Classifications - Type of county (rural or urban on a rural-urban continuum scale).
3. [Intervention Policy Data](https://github.com/JieYingWu/COVID-19_US_County-level_Summaries/blob/master/data/interventions.csv) - This dataset is a manually compiled list of the dates that interventions/lockdown policies were implemented and lifted at the county level. 

# EDA

In this case study, we use the following three nearly cleaned data:

* **covid_county.csv**: County-level socialeconomic information that combines the above-mentioned 4 datasets: Income (Poverty level and household income), Jobs (Employment type, rate, and change), People (Population size, density, education level, race, age, household size, and migration rates), County Classifications
* **covid_rates.csv**: Daily cumulative numbers on infection and fatality for each county
* **covid_intervention.csv**: County-level lockdown intervention.

Among all data, the unique identifier of county is `FIPS`.

The cleaning procedure is attached in `Appendix 2: Data cleaning` You may go through it if you are interested or would like to make any changes. 

**It may need more data wrangling.**

First read in the data.

```{r}
# county-level socialeconomic information
county_data <- fread("data/covid_county.csv") 
# county-level COVID case and death
covid_rate <- fread("data/covid_rates.csv")
# county-level lockdown dates 
# covid_intervention <- fread("data/covid_intervention.csv")
```

## Understand the data

The detailed description of variables is in `Appendix 1: Data description`. Please get familiar with the variables. Summarize the two data briefly.

```{r}
# explore county data
head(county_data)
dim(county_data)
# str(county_data)
# summary(county_data)
skim(county_data)

# explore covid_rate
head(covid_rate)
dim(covid_rate)
# str(covid_rate)
# summary(covid_rate)
skim(covid_rate)

```
  
Based on this summary, we note that:

* A number of variables in the `county_data` contain NAs.
* The `covid_rate` dataset does not contain any NAs.
* County and State are character variables; we will change these to factors for further analyses. 

```{r}
# change county and state to factors in both datasets
county_data <- county_data %>%
  mutate(State = as.factor(State),
         County = as.factor(County))

covid_rate <- covid_rate %>%
  mutate(State = as.factor(State),
         County = as.factor(County))

```



## COVID case trend

It is crucial to decide the right granularity for visualization and analysis. We will compare daily vs weekly total new cases by state and we will see it is hard to interpret daily report.

i) Plot **new** COVID cases in NY, WA and FL by state and by day. Any irregular pattern? What is the biggest problem of using single day data? 

First, we start by creating a `county_new_cases` variable, defined as the row-wise difference of the existing cumulative variable `cum_cases`. Next, we create a `state_new_cases` variable, which is the sum of all new cases per day, across all counties of a given state. 

```{r}

# some cum_cases are acutally decreasing, which is not possible for a cumulative variable. When a given cum_case value is smaller than the previous cum_case value, we replace that cum_case value by the previous value: 
# covid_rate <- covid_rate %>%
  # mutate(cum_cases = ifelse(cum_cases < lag(cum_cases), lag(cum_cases), cum_cases)) # %>%
  # mutate(cum_cases = ifelse(cum_cases < lag(cum_cases), lag(cum_cases), cum_cases)) %>%
  # mutate(cum_cases = ifelse(cum_cases < lag(cum_cases), lag(cum_cases), cum_cases)) %>%
  # mutate(cum_cases = ifelse(cum_cases < lag(cum_cases), lag(cum_cases), cum_cases))
# ended up not doing this as there are many decreasing values in a row, which means I'd have to replace by a previous value repeatedly for all those rows.

# create county_new_cases variable
covid_rate <- covid_rate %>%
  group_by(County, State) %>%
  mutate(county_new_cases = cum_cases - lag(cum_cases))  %>%
  mutate(county_new_cases = ifelse(is.na(county_new_cases), cum_cases, county_new_cases)) # the first new_cases entry for every county is an NA as there was no prior data point to compute a difference with. Thus, we replace NAs with the first reported number of cases that day. 

# let's see if this new variable seems right
summary(covid_rate$county_new_cases)

# some values are  negative! Let's see which: 
covid_rate[which(covid_rate$county_new_cases < -1000), ]

covid_rate[covid_rate$County == "Middlesex" & covid_rate$State == "Massachusetts" & (covid_rate$date == "2020-09-01" | covid_rate$date == "2020-09-02"), ]

# it looks like some counties have large "jumps" in their cumulative cases, for instance in MS, Middlesex county reports 27732 on 09-01, but then 25411 the next day, which leads to a difference of -2321! We suspect this is a reporting error, or a correction for a number of suspected cases which turned out not be COVID? 

# To correct for this in the county_new_cases, we replace all negative values by 0, indicating no new cases that day. 
covid_rate$county_new_cases <- ifelse((covid_rate$county_new_cases) < 0, 
                                      0, # if value is smaller than 0, attribute 0 to this cell
                                      covid_rate$county_new_cases) # else, just report the current value

summary(covid_rate$county_new_cases) # values seem reasonable now.
covid_rate[covid_rate$County == "Middlesex" & covid_rate$State == "Massachusetts", ]

# this provides the number of new cases per county though, and we need to know the total sum of new daily cases per state.
# So let's create a state_new_cases variable
daily_state <- covid_rate %>%
  group_by(State, date) %>%
  summarise(
    state_new_cases = sum(county_new_cases, na.rm = TRUE)
  )
  # mutate(state_new_cases = sum(county_new_cases, na.rm = TRUE))

# let's see if this new variable seems right
summary(daily_state$state_new_cases) # seems reasonable, no negative values
tmp <- covid_rate[covid_rate$date == "2020-03-13" & covid_rate$State == "Alabama", ]
sum(tmp$county_new_cases)
# the variable seems correct.

```


```{r fig.width=10}
# plot new daily covid cases in NY, WA and FL
daily_state %>%
  filter(State %in% c("New York", "Washington", "Florida")) %>%
  ggplot(aes(x = date, y = state_new_cases, color = State)) + 
    geom_point() + 
    facet_grid(~State) +
    theme_bw() +
    ggtitle("New daily COVID cases in Florida, New York and Washington")


# plot the cum_cases for a few counties to illustrate "breaks" in the cumulative variable (this should technically not be possible)
PA <- covid_rate %>%
  filter(State %in% c("Pennsylvania"),
         County %in% c("Lancaster", "Allegheny", "Philadelphia")) %>%
  ggplot(aes(x = date, y = cum_cases, color = County)) + 
    geom_point() + 
    facet_grid(~County) + 
    theme_bw() +
    ggtitle("New daily COVID cases in 3 counties of Pennsylvania")

MS <- covid_rate %>%
  filter(State %in% c("Massachusetts"),
         County %in% c("Suffolk", "Barnstable", "Middlesex")) %>%
  ggplot(aes(x = date, y = cum_cases, color = County)) + 
    geom_point() + 
    facet_grid(~County) + 
    theme_bw() +
    ggtitle("New daily COVID cases in 3 counties of Massachusetts")

ggpubr::ggarrange(PA, MS, nrow = 2)


# plot the county_new_cases for a few counties to illustrate negative values (this should technically not be possible)
# PA <- covid_rate %>%
#   filter(State %in% c("Pennsylvania"),
#          County %in% c("Lancaster", "Allegheny", "Philadelphia")) %>%
#   ggplot(aes(x = date, y = county_new_cases, color = County)) + 
#     geom_point() + 
#     facet_grid(~County) + 
#     theme_bw() +
#     ggtitle("New daily COVID cases in 3 counties of Pennsylvania")
# 
# MS <- covid_rate %>%
#   filter(State %in% c("Massachusetts"),
#          County %in% c("Suffolk", "Barnstable", "Middlesex")) %>%
#   ggplot(aes(x = date, y = county_new_cases, color = County)) + 
#     geom_point() + 
#     facet_grid(~County) + 
#     theme_bw() +
#     ggtitle("New daily COVID cases in 3 counties of Massachusetts")
# 
# ggpubr::ggarrange(PA, MS, nrow = 2)

```


When using single day data, there is more variability in the number of new cases, making the trends harder to see. For instance, some days have close to 0 new cases, while other days contains up to 20000 new cases (Florida in January 2021). Thus, looking at the evolution of cases on a daily level may be too granular to get a good sense of COVID trends over time.  

ii) Create **weekly new** cases per 100k `weekly_case_per100k`. Plot the spaghetti plots of `weekly_case_per100k` by state. Use `TotalPopEst2019` as population.

We want to know the number of new cases per week and per 100k in each state. To do this, we sum the new cases up per state and week, divide it by the total population and then by 100'000. 

```{r fig.width=8}

# create total state population variable
total_pop <- covid_rate %>%
  group_by(County, State) %>%
  filter(row_number() == 1) %>%
  group_by(State) %>%
  mutate(state_pop = sum(TotalPopEst2019)) %>%
  select(FIPS, state_pop)

# merge to main dataframe
covid_rate <- merge(covid_rate, total_pop, all.x= T)

# create weekly new cases per state and per 100k
weekly_state <- covid_rate %>%
  group_by(State, week) %>%
  summarise(
    state_weekly_new = sum(county_new_cases, na.rm = TRUE), 
    weekly_case_per100k = state_weekly_new / (first(state_pop) / 100000)
  )


# spaghetti plot of weekly_case_per100k for all States
weekly_state %>%
  ggplot(aes(x = week, y = weekly_case_per100k, color = State)) + 
    geom_line() + 
    theme_bw() +
    ggtitle("Weekly new cases per 100k in all States") +
    theme(legend.position="bottom")

# spaghetti plot of weekly_case_per100k for NY, FL, WA
weekly_state %>%
  filter(State %in% c("New York", "Washington", "Florida")) %>%
  ggplot(aes(x = week, y = weekly_case_per100k, color = State)) + 
    geom_line(size = 2) + 
    theme_bw() +
    ggtitle("Weekly new cases per 100k in Florida, New York and Washington") +
    theme(legend.position="bottom")

```


iii) Summarize the COVID case trend among states based on the plot in ii). What could be the possible reasons to explain the variabilities? 

In general, across all States we note 3-4 major waves across this time period: 
* Around week 10, corresponding to end of March, 2020
* Around week 25, corresponding to early July, 2020
* Around week 42 (major peak), corresponding to early November 2020
* Around week 46, corresponding to early December 2020

When plotting Washington, Florida and New York only, we note that while all 3 States had a wave of COVID cases in early December, New York had its own wave around week 10-11 (end of March, 2020) and Florida had its own wave around week 25-26 (July 2020).

These observations are likely due to a combination of changes in restrictions as well as major holidays and tourism/population flow. For instance, New York may have been initially more heavily impacted at the start of the pandemic in March 2020 due to the high number of travellers passing through NYC. Florida was more affected in summer, as its beaches attract more tourists. Finally, all States were impacted in early December as a consequence of gatherings during Thanksgiving. 

iv) (Optional) Use `covid_intervention` to see whether the effectiveness of lockdown in flattening the curve.

## COVID death trend

i) For each month in 2020, plot the monthly deaths per 100k heatmap by state on US map. Use the same color range across months. (Hints: Set `limits` argument in `scale_fill_gradient()` or use `facet_wrap()`; use `lubridate::month()` and `lubridate::year()` to extract month and year from date; use `tidyr::complete(state, month, fill = list(new_case_per100k = NA))` to complete the missing months with no cases.)

```{r, fig.width=15}
# I'll first create a month and year variable for each row. This will help us pick out what we need.
covid_rate <- covid_rate %>%
  mutate(YEAR = lubridate::year(date)) %>%
  mutate(MONTH = lubridate::month(date))
covid_rate$MONTH <- month.abb[covid_rate$MONTH] # convert to month names

# Here I extract just the data from 2020
covid_rate_2020 <- covid_rate[covid_rate$YEAR == 2020, ]

# Create a daily deaths per 
covid_rate_2020 <- covid_rate_2020 %>%
  group_by(County, State) %>%
  mutate(county_new_deaths = cum_deaths - lag(cum_deaths))  %>%
  mutate(county_new_deaths = ifelse(is.na(county_new_deaths), cum_deaths, county_new_deaths)) # the first county_new_deaths entry for every county is an NA as there was no prior data point to compute a difference with. Thus, we replace NAs with the first reported number of deaths that day. 

summary(covid_rate_2020$county_new_deaths) # again, we see that some are negative - this is due to later cum_deaths being less than preceding ones. This is likely due to correcting misreporting or errors in data entry. 
# To account for this, set negative values to zero. 
covid_rate_2020$county_new_deaths <- ifelse((covid_rate_2020$county_new_deaths) < 0, 
                                      0, # if value is smaller than 0, attribute 0 to this cell
                                      covid_rate_2020$county_new_deaths) # else, just report the current value
summary(covid_rate_2020$county_new_deaths) # looks like we're in good shape!

# let's now get deaths per 100k population
covid_rate_2020 <- covid_rate_2020 %>% 
  mutate(deaths_per_100k = county_new_deaths/(state_pop/100000))

# now, let's create a new data table with just new deaths per month by state (rows: state, columns: months)
covid_deaths_2020 <- covid_rate_2020 %>% 
  group_by(State, MONTH) %>%
  summarize(state_deaths_per_100k = sum(deaths_per_100k))
covid_deaths_2020$MONTH <- factor(covid_deaths_2020$MONTH, levels = c(month.abb[1:12])) # make months factors and reorder. This will help later when using facet_wrap so that months are displayed in order

max_deaths <- max(covid_deaths_2020$state_deaths_per_100k) # get the upper limit for our color gradient

# filling missing months with NA
covid_deaths_2020 <- covid_deaths_2020 %>% 
  group_by(State) %>% 
  complete(MONTH) %>% 
  ungroup()

# now we can generate our heatmaps
colnames(covid_deaths_2020)[1] = 'state' # lowercase to match what plot_usmap expects

# create a plot for month
covid_map <- plot_usmap(data = covid_deaths_2020, values = "state_deaths_per_100k", regions = 'states', color = 'black', include = unique(covid_deaths_2020$state)) + 
  scale_fill_continuous(low = "white", high = "red4", name = "COVID-19 Deaths per 100k", limits = c(0, max_deaths)) + 
  labs(title = "COVID-19 Deaths per 100k Month-by-Month") + 
  theme(legend.position = "right") + facet_wrap(vars(MONTH))
covid_map

# ggplotly(covid_map, text = paste("Cases:", NAME_1))
  
```

ii) (Optional) Use `plotly` to animate the monthly maps in i). Does it reveal any systematic way to capture the dynamic changes among states? (Hints: Follow *Appendix 3: Plotly heatmap::* in Module 6 regularization lecture to plot the heatmap using `plotly`. Use `frame` argument in `add_trace()` for animation. `plotly` only recognizes abbreviation of state names. Use `unique(us_map(regions = "states") %>% select(abbr, full))` to get the abbreviation and merge with the data to get state abbreviation.)


```{r results='show', warning=FALSE, cache=FALSE}

# get US state abbreviations
state_abbr <- unique(us_map(regions = "states") %>% select(abbr, full))

# rename full to state
state_abbr <- state_abbr %>%
  rename(state = full)

# merge with main dataframe
covid_deaths_2020 <- merge(covid_deaths_2020,
             state_abbr,
             all.x = T)

# add a variable hover to store the state description as text when you hover on a state
# <br> means new line in HTML
plotting_info <- covid_deaths_2020 %>% 
  mutate(hover =  paste(state, '<br>',
                        'Deaths per 100k: ', round(state_deaths_per_100k, 1)))
# give state boundaries a white border
l <- list(color = toRGB("white"), width = 2)
# specify some map projection/options
g <- list(
  scope = 'usa',
  projection = list(type = 'albers usa'),
  showlakes = FALSE
)

fig <- plot_geo(plotting_info, locationmode = 'USA-states')
fig <- fig %>% add_trace(
    z = ~state_deaths_per_100k, text = ~hover, locations = ~abbr,
    color = ~state_deaths_per_100k, colors = 'Reds', 
    zauto = FALSE, zmin = 0, zmax = max_deaths,
    frame = ~MONTH
  )

fig <- fig %>% layout(
    title = 'COVID-19 Deaths per 100k',
    geo = g,
    hoverlabel = list(bgcolor = "black")
  )

fig
```

It does reveal how the COVID wave spreads primarily from states with high international travel and large urban centers (i.e NYC), through neighboring states, and settles in rural states last. 


# COVID factor

We now try to build a good parsimonious model to find possible factors related to death rate on county level. Let us not take time series into account for the moment and use the total number as of *Feb 1, 2021*.

i) Create the response variable `total_death_per100k` as the total of number of COVID deaths per 100k by *Feb 1, 2021*. We suggest to take log transformation as `log_total_death_per100k = log(total_death_per100k + 1)`. Merge `total_death_per100k` to `county_data` for the following analysis.

```{r}
# let's create a response variable. We'll use the log transformed deaths in our analysis. 
# get just data for Feb 1 2021 
feb1_data <- covid_rate[covid_rate$date == as.Date("2021-02-01"), ]

# create a total_death_per100k variable. Here we'll be using county population to make our calculation, as we're working at the county level. 
feb1_data <- feb1_data %>%
  mutate(total_death_per100k = cum_deaths/(TotalPopEst2019/100000))
# now, we can derive a log transform of our variable
feb1_data <- feb1_data %>%
  mutate(log_total_death_per100k = log(total_death_per100k + 1))

# Append to the county_data data.frame. We'll use the FIPS code to link the new variables to the correct county.
county_data <- merge(x=county_data, y=feb1_data[, c("FIPS", "total_death_per100k", "log_total_death_per100k")], by="FIPS", all.x=TRUE)

summary(county_data[, c("total_death_per100k", "log_total_death_per100k")]) # let's take a look at our new appended variables ot make sure everything is ok. Looks like we have some NAs, though this is expected because there are more counties in county_data than in covid_rate. 
length(unique(covid_rate$FIPS)) < length(county_data$FIPS)

```

ii) Select possible variables in `county_data` as covariates. We provide `county_data_sub`, a subset variables from `county_data`, for you to get started. Please add any potential variables as you wish. 

    a) Report missing values in your final subset of variables. 

    b) In the following anaylsis, you may ignore the missing values.

We are adding AvgHHSize, indicating the average household size, as crowdedness within households may potentially have an influence on COVID spreading. 

```{r}
county_data_sub <- county_data %>%
  select(County, State, FIPS, # general info
         Deep_Pov_All, PovertyAllAgesPct, PerCapitaInc, # income/poverty
         UnempRate2019, PctEmpFIRE, PctEmpConstruction, PctEmpTrans, PctEmpMining, PctEmpTrade, PctEmpInformation, PctEmpAgriculture, PctEmpManufacturing, PctEmpServices, # employment
         PopDensity2010, OwnHomePct, AvgHHSize, # housing (adding AvgHHSize here as crowdedness may have an influence on COVID spreading?)
         Age65AndOlderPct2010, TotalPop25Plus, Under18Pct2010, # age
         Ed2HSDiplomaOnlyPct, Ed3SomeCollegePct, Ed4AssocDegreePct, Ed5CollegePlusPct, # education
         ForeignBornPct, Net_International_Migration_Rate_2010_2019, NetMigrationRate1019, # migration
         NaturalChangeRate1019, TotalPopEst2019, # population size
         WhiteNonHispanicPct2010, NativeAmericanNonHispanicPct2010, BlackNonHispanicPct2010, AsianNonHispanicPct2010, HispanicPct2010, # race & ethnicity
         Type_2015_Update, RuralUrbanContinuumCode2013, UrbanInfluenceCode2013, # county type (rural vs urban)
         Perpov_1980_0711, # persistent poverty counties
         HiCreativeClass2000, HiAmenity, Retirement_Destination_2015_Update, # other
         log_total_death_per100k) # dependent variable

# get a sense of the data subset
skim(county_data_sub)

```

Most variables have a number of missing values (ranging from around 5-170 NAs). As instructed, we will ignore these in building our model.  

iii) Use LASSO to choose a parsimonious model with all available sensible county-level information. **Force in State** in the process. Why we need to force in State? You may use `lambda.1se` to choose a smaller model. 

State should be forced into the model because this is a critical variable of interest in our study. To avoid potentially discarding it, we use the `penalty.factor` argument from `cv.glmnet` to not impose any penalty on the coefficients of this variable.  

```{r}

# remove county variable as we don't want to include this in the model
county_data_sub <- county_data_sub %>%
  select(-County, -FIPS)

county_data_sub$Type_2015_Update <- as.factor(county_data_sub$Type_2015_Update) # set this variable to be a factor

# remove all rows containing NAs (LASSO does not work without this step)
county_data_sub <- na.omit(county_data_sub)

# create matrices to feed into gmnet
Y <- as.matrix(county_data_sub[, 42]) # extract Y (log_total_death_per100k)
X <- model.matrix(log_total_death_per100k~., data = county_data_sub)[, -1] # remove the first (interecept) column with only 1s

# checking data types (should be matrices)
# typeof(X)
# typeof(Y)

# to control the randomness in K folds 
set.seed(10)  

# building the penalty factor
length(levels(county_data_sub$State))
ncol(X)
length(c(rep(0, 53), rep(1, ncol(X)-53))) # repeat 0 for each of the 53 States (not penalized), repeat 1 for other variables (penalized)

# run LASSO
lasso_cv <- cv.glmnet(X, Y, alpha = 1, nfolds = 10, intercept = TRUE,
                       penalty.factor = c(rep(0, 53), rep(1, ncol(X)-53)))  

# plot LASSO results
plot(lasso_cv)

lasso_cv$lambda.1se # use lambda.1se to select the smaller model
coef.1se <- coef(lasso_cv, s = "lambda.1se") # get coefficients
coef.1se <- coef.1se[which(coef.1se != 0),] # show variables with non-zero coefficients

# output variable names
coef.1se <- rownames(as.matrix(coef.1se))[-1] 
coef.1se

```

iv) Use `Cp` or BIC to fine tune the LASSO model from iii). Again **force in State** in the process. 

Running regsubsets with State forced in causes a crash on the computers of all group members. To still be able to account for State effects, we ran regsubsets without State, and included it in our final model as a workaroud "forcing" of State. 

```{r}
# let's first see what the linear model looks like based on variables selected by LASSO
coef.1se <- coef(lasso_cv, s="lambda.1se")  #s=c("lambda.1se","lambda.min") or lambda value
coef.1se <- coef.1se[which(coef.1se != 0),]   # get the non=zero coefficients
var.1se <- rownames(as.matrix(coef.1se))[-1] # output the variable names without intercept
var.1se <- var.1se[-grep("State", var.1se)] # remove the state levels
county_data_lasso <-  county_data_sub %>%
  select(c("State", "log_total_death_per100k", var.1se)) # get a subset with response and LASSO output

# run relaxed LASSO
fit.1se.lm <- lm(log_total_death_per100k~., data = county_data_lasso) 
summary(fit.1se.lm) 

### TO DO: use Cp or BIC to fine-tune the LASSO model.
county_data_lasso <- data.frame(county_data_lasso) # convert from data.table to data.frame
county_data_lasso_sub <- county_data_lasso[ ,c("log_total_death_per100k", var.1se)] # extract response & LASSO output vars
county_data_lasso_fine_tune <- regsubsets(log_total_death_per100k ~., nvmax = 17, method = "exhau", county_data_lasso_sub) # get Cp and BIC
# Plot the Cp and BIC
plot(summary(county_data_lasso_fine_tune)$cp)
plot(summary(county_data_lasso_fine_tune)$bic)

optimal_size = 9 # 9 Seems to be a good number 
final_variables <- colnames(summary(county_data_lasso_fine_tune)$which)[summary(county_data_lasso_fine_tune)$which[optimal_size, ]][-1] # extract the variables in the optimal model, minus the intercept
county_data_sub <- data.frame(county_data_sub) # convert to data.frame from data.table
final_covid_fit <- lm(log_total_death_per100k ~., data = data.frame(county_data_lasso[, c("log_total_death_per100k", final_variables, 'State')]))
summary(final_covid_fit)
Anova(final_covid_fit)
```


v) If necessary, reduce the model from iv) to a final model with all variables being significant at 0.05 level. Are the linear model assumptions all reasonably met?
```{r}
plot(final_covid_fit)
```

The asusmptions of the model are not met. This is likely due to the influence of outliers. We will identify and remove outliers from our model to achieve a better fit. We define outliers as having a residual more than two sigma away from the mean. 

outliers indentify
```{r}
county_data_lasso$Residuals<-final_covid_fit$resid
county_data_lasso_fix <- county_data_lasso%>%
  subset(abs(Residuals) < 2*sigma(final_covid_fit))

```

# remove outliers
```{r}
#county_data_lasso_fix <- county_data_lasso%>%
#  subset(log_total_death_per100k != 0)
  
final_covid_fit_nozero <- lm(log_total_death_per100k ~., data = data.frame(county_data_lasso_fix[, c("log_total_death_per100k", final_variables, 'State')]))
summary(final_covid_fit_nozero)
Anova(final_covid_fit_nozero)
plot(final_covid_fit_nozero)

```

reduced model
```{r}
#final_covid_fit_reduced <- lm(log_total_death_per100k ~., data = data.frame(county_data_lasso_fix[, c("log_total_death_per100k", final_variables[-2], 'State')]))
#anova(final_covid_fit_reduced, final_covid_fit_nozero)
```

model assumptions
```{r}
#plot(final_covid_fit_reduced)
plot(final_covid_fit_nozero)
```

Now that we have removed outliers, model assumptions are reasonably met. 

vi) It has been shown that COVID affects elderly the most. It is also claimed that the COVID death rate among African Americans and Latinos is higher. Does your analysis support these arguments?

Our model does not explicitly/directly suggest that the death rate is higher in African Americans and Latinos. Instead, we can see that being white non-histpanic is associated with a slightly lower mortality rate. This suggests that all minority groups fare worse in terms of mortality. 

vii) Based on your final model, summarize your findings. In particular, summarize the state effect controlling for others. Provide intervention recommendations to policy makers to reduce COVID death rate.

The state effect appears to be quite important, with several states being associated with higher mortality (e.g. South Dakota), and several being associated with lower mortality (e.g. Vermont). It is our recommendation that interventions should be handled on a state-by-state basis, and federally determined policies may not be beneficial across the board. Additionally, it appears that age is a major factor, with higher numbers of children and people over the age of 65 both being associated with a higher mortality. The effect of children is likely due to the fact that vaccine approval for children required more time, as well as schooling and play serving as possible transmission vectors. Of course, since individuals over the age of 65 are at higher risk of death from COVID due to immune function decay, it is not surprising that higher amounts of these individuals are associated with higher mortality. Policy interventions may focus on protecting individuals over the age of 65 (via healthcare subsidies and facility availability), and making vaccines more available among children now that approval has been cleared. Higher education is associated with reduced mortality, indicating a possible intervention route as increasing higher education in some capacity, possibly via subsidies and incentives programs. The percent of people employed in construction and agriculture are both associated with lower mortality, which may be surprising at first, but both of these trades routinely require the use of face masks despite elevated levels of person-to-person contact, which may explain some of the effect. This may imply that face masks are indeed effective at reducing transmission. The economic type of a county also seems to affect mortality, though thi smay be captured by the percent in construction and agriculture. Percent of white non-hispanic and net migration rate between 2010-2019 are associated with reduced mortality, indicating that all minority groups or immigrant populations may be at higher risk, and interventions should be crafted around improving the conditions of minority or foreign groups. 

viii) What else can we do to improve our model? What other important information we may have missed? 

The following information may be helpful in improving out model:

1. Hospital Load & Healthcare Infrastructure Data (i.e. capacity of hospitals and care facilities per county/state)
2. Political Party Information (percent of voters registered in each party)
3. Holidays & Gatherings (travel information for individuals)

ix) (Optional) Would your findings be very different if you had refined the data in some way or imputed the missing values in part ii). Check PCA lecture, section 10 for imputations via `softImpute`. 


# Appendix 1: Data description {-}

A detailed summary of the variables in each data set follows:

**Infection and fatality data**

* date: Date
* county: County name
* state: State name
* fips: County code that uniquely identifies a county
* cases: Number of cumulative COVID-19 infections
* deaths: Number of cumulative COVID-19 deaths

**Socioeconomic demographics**

*Income*: Poverty level and household income 

* PovertyUnder18Pct: Poverty rate for children age 0-17, 2018  
* Deep_Pov_All: Deep poverty, 2014-18  
* Deep_Pov_Children: Deep poverty for children, 2014-18  
* PovertyAllAgesPct: Poverty rate, 2018  
* MedHHInc: Median household income, 2018 (In 2018 dollars)  
* PerCapitaInc: Per capita income in the past 12 months (In 2018 inflation adjusted dollars), 2014-18  
* PovertyAllAgesNum: Number of people of all ages in poverty, 2018  
* PovertyUnder18Num: Number of people age 0-17 in poverty, 2018   

*Jobs*: Employment type, rate, and change 

* UnempRate2007-2019: Unemployment rate, 2007-2019 
* NumEmployed2007-2019: Employed, 2007-2019
* NumUnemployed2007-2019: Unemployed, 2007-2019

* PctEmpChange1019: Percent employment change, 2010-19  
* PctEmpChange1819: Percent employment change, 2018-19  
* PctEmpChange0719: Percent employment change, 2007-19  
* PctEmpChange0710: Percent employment change, 2007-10  

* NumCivEmployed: Civilian employed population 16 years and over, 2014-18  
* NumCivLaborforce2007-2019: Civilian labor force, 2007-2019  


* PctEmpFIRE: Percent of the civilian labor force 16 and over  employed in finance and insurance, and real estate and rental and leasing, 2014-18    
* PctEmpConstruction: Percent of the civilian labor force 16 and over employed in construction, 2014-18  
* PctEmpTrans: Percent of the civilian labor force 16 and over employed in transportation, warehousing and utilities, 2014-18 
* PctEmpMining: Percent of the civilian labor force 16 and over employed in mining, quarrying, oil and gas extraction, 2014-18  
* PctEmpTrade: Percent of the civilian labor force 16 and over employed in wholesale and retail trade, 2014-18  
* PctEmpInformation: Percent of the civilian labor force 16 and over employed in information services, 2014-18   
* PctEmpAgriculture: Percent of the civilian labor force 16 and over employed in agriculture, forestry, fishing, and hunting, 2014-18   
* PctEmpManufacturing: Percent of the civilian labor force 16 and over employed in manufacturing, 2014-18    
* PctEmpServices: Percent of the civilian labor force 16 and over employed in services, 2014-18    
* PctEmpGovt: Percent of the civilian labor force 16 and over employed in public administration, 2014-18 

*People*: Population size, density, education level, race, age, household size, and migration rates

* PopDensity2010: Population density, 2010  
* LandAreaSQMiles2010: Land area in square miles, 2010 

* TotalHH: Total number of households, 2014-18  
* TotalOccHU: Total number of occupied housing units, 2014-18  
* AvgHHSize: Average household size, 2014-18   
* OwnHomeNum: Number of owner occupied housing units, 2014-18  
* OwnHomePct: Percent of owner occupied housing units, 2014-18
* NonEnglishHHPct: Percent of non-English speaking households of total households, 2014-18     
* HH65PlusAlonePct: Percent of persons 65 or older living alone, 2014-18  
* FemaleHHPct: Percent of female headed family households of total households, 2014-18  
* FemaleHHNum: Number of female headed family households, 2014-18  
* NonEnglishHHNum: Number of non-English speaking households, 2014-18  
* HH65PlusAloneNum: Number of persons 65 years or older living alone, 2014-18

* Age65AndOlderPct2010: Percent of population 65 or older, 2010
* Age65AndOlderNum2010: Population 65 years or older, 2010  
* TotalPop25Plus: Total population 25 and older, 2014-18 - 5-year average  
* Under18Pct2010: Percent of population under age 18, 2010  
* Under18Num2010: Population under age 18, 2010

*  Ed1LessThanHSPct: Percent of persons with no high school diploma or GED, adults 25 and over, 2014-18  
* Ed2HSDiplomaOnlyPct: Percent of persons with a high school diploma or GED only, adults 25 and over, 2014-18    
* Ed3SomeCollegePct: Percent of persons with some college experience, adults 25 and over, 2014-18    
* Ed4AssocDegreePct: Percent of persons with an associate's degree, adults 25 and over, 2014-18    
* Ed5CollegePlusPct: Percent of persons with a 4-year college degree or more, adults 25 and over, 2014-18    
* Ed1LessThanHSNum: No high school, adults 25 and over, 2014-18
* Ed2HSDiplomaOnlyNum: High school only, adults 25 and over, 2014-18  
* Ed3SomeCollegeNum: Some college experience, adults 25 and over, 2014-18   
* Ed4AssocDegreeNum: Number of persons with an associate's degree, adults 25 and over, 2014-18   
* Ed5CollegePlusNum: College degree 4-years or more, adults 25 and over, 2014-18   

* ForeignBornPct: Percent of total population foreign born, 2014-18  
* ForeignBornEuropePct: Percent of persons born in Europe, 2014-18  
* ForeignBornMexPct: Percent of persons born in Mexico, 2014-18
* ForeignBornCentralSouthAmPct: Percent of persons born in Central or South America, 2014-18  
* ForeignBornAsiaPct: Percent of persons born in Asia, 2014-18
* ForeignBornCaribPct: Percent of persons born in the Caribbean, 2014-18  
* ForeignBornAfricaPct: Percent of persons born in Africa, 2014-18   
* ForeignBornNum: Number of people foreign born, 2014-18  
* ForeignBornCentralSouthAmNum: Number of persons born in Central or South America, 2014-18   
* ForeignBornEuropeNum: Number of persons born in Europe, 2014-18  
* ForeignBornMexNum: Number of persons born in Mexico, 2014-18
* ForeignBornAfricaNum: Number of persons born in Africa, 2014-18  
* ForeignBornAsiaNum: Number of persons born in Asia, 2014-18 
* ForeignBornCaribNum: Number of persons born in the Caribbean, 2014-18   

* Net_International_Migration_Rate_2010_2019: Net international migration rate, 2010-19  
* Net_International_Migration_2010_2019: Net international migration, 2010-19   
* Net_International_Migration_2000_2010: Net international migration, 2000-10  
* Immigration_Rate_2000_2010: Net international migration rate, 2000-10   
* NetMigrationRate0010: Net migration rate, 2000-10   
* NetMigrationRate1019: Net migration rate, 2010-19  
* NetMigrationNum0010: Net migration, 2000-10  
* NetMigration1019: Net Migration, 2010-19  

* NaturalChangeRate1019: Natural population change rate, 2010-19   
* NaturalChangeRate0010: Natural population change rate, 2000-10    
* NaturalChangeNum0010: Natural change, 2000-10  
* NaturalChange1019: Natural population change, 2010-19

* TotalPop2010: Population size 4/1/2010 Census 
* TotalPopEst2010: Population size 7/1/2010
* TotalPopEst2011: Population size 7/1/2011
* TotalPopEst2012: Population size 7/1/2012
* TotalPopEst2013: Population size 7/1/2013
* TotalPopEst2014: Population size 7/1/2014
* TotalPopEst2015: Population size 7/1/2015
* TotalPopEst2016: Population size 7/1/2016
* TotalPopEst2017: Population size 7/1/2017
* TotalPopEst2018: Population size 7/1/2018
* TotalPopEst2019: Population size 7/1/2019
* TotalPopACS: Total population, 2014-18 - 5-year average   
* TotalPopEstBase2010: County Population estimate base 4/1/2010

* NonHispanicAsianPopChangeRate0010: Population change rate Non-Hispanic Asian, 2000-10  
* PopChangeRate1819: Population change rate, 2018-19    
* PopChangeRate1019: Population change rate, 2010-19    
* PopChangeRate0010: Population change rate, 2000-10   
* NonHispanicNativeAmericanPopChangeRate0010: Population change rate Non-Hispanic Native American, 2000-10    
* HispanicPopChangeRate0010: Population change rate Hispanic, 2000-10  
* MultipleRacePopChangeRate0010: Population change rate multiple race, 2000-10    
* NonHispanicWhitePopChangeRate0010: Population change rate Non-Hispanic White, 2000-10  
* NonHispanicBlackPopChangeRate0010: Population change rate Non-Hispanic African American, 2000-10  

* MultipleRacePct2010: Percent multiple race, 2010  
* WhiteNonHispanicPct2010: Percent Non-Hispanic White, 2010    
* NativeAmericanNonHispanicPct2010: Percent Non-Hispanic Native American, 2010  
* BlackNonHispanicPct2010: Percent Non-Hispanic African American, 2010    
* AsianNonHispanicPct2010: Percent Non-Hispanic Asian, 2010   
* HispanicPct2010: Percent Hispanic, 2010  
* MultipleRaceNum2010: Population size multiple race, 2010   
* WhiteNonHispanicNum2010: Population size Non-Hispanic White, 2010    
* BlackNonHispanicNum2010: Population size Non-Hispanic African American, 2010  
* NativeAmericanNonHispanicNum2010: Population size Non-Hispanic Native American, 2010   
* AsianNonHispanicNum2010: Population size Non-Hispanic Asian, 2010    
* HispanicNum2010: Population size Hispanic, 2010

*County classifications*: Type of county (rural or urban on a rural-urban continuum scale)

* Type_2015_Recreation_NO: Recreation counties, 2015 edition  
* Type_2015_Farming_NO: Farming-dependent counties, 2015 edition  
* Type_2015_Mining_NO: Mining-dependent counties, 2015 edition
* Type_2015_Government_NO: Federal/State government-dependent counties, 2015 edition  
* Type_2015_Update: County typology economic types, 2015 edition   
* Type_2015_Manufacturing_NO: Manufacturing-dependent counties, 2015 edition  
* Type_2015_Nonspecialized_NO: Nonspecialized counties, 2015 edition    
* RecreationDependent2000: Nonmetro recreation-dependent, 1997-00  
* ManufacturingDependent2000: Manufacturing-dependent, 1998-00
* FarmDependent2003: Farm-dependent, 1998-00  
* EconomicDependence2000: Economic dependence, 1998-00  

* RuralUrbanContinuumCode2003: Rural-urban continuum code, 2003
* UrbanInfluenceCode2003: Urban influence code, 2003  
* RuralUrbanContinuumCode2013: Rural-urban continuum code, 2013
* UrbanInfluenceCode2013: Urban influence code, 2013  
* Noncore2013: Nonmetro noncore, outside Micropolitan and Metropolitan, 2013  
* Micropolitan2013: Micropolitan, 2013
* Nonmetro2013: Nonmetro, 2013
* Metro2013: Metro, 2013  
* Metro_Adjacent2013: Nonmetro, adjacent to metro area, 2013  
* Noncore2003: Nonmetro noncore, outside Micropolitan and Metropolitan, 2003  
* Micropolitan2003: Micropolitan, 2003  
* Metro2003: Metro, 2003  
* Nonmetro2003: Nonmetro, 2003  
* NonmetroNotAdj2003: Nonmetro, nonadjacent to metro area, 2003
* NonmetroAdj2003: Nonmetro, adjacent to metro area, 2003  

* Oil_Gas_Change: Change in the value of onshore oil and natural gas production, 2000-11  
* Gas_Change: Change in the value of onshore natural gas production, 2000-11    
* Oil_Change: Change in the value of onshore oil production, 2000-11  

* Hipov: High poverty counties, 2014-18  
* Perpov_1980_0711: Persistent poverty counties, 2015 edition  
* PersistentChildPoverty_1980_2011: Persistent child poverty counties, 2015 edition  
* PersistentChildPoverty2004: Persistent child poverty counties, 2004  
* PersistentPoverty2000: Persistent poverty counties, 2004

* Low_Education_2015_update: Low education counties, 2015 edition
* LowEducation2000: Low education, 2000  

* HiCreativeClass2000: Creative class, 2000  
* HiAmenity: High natural amenities  
* RetirementDestination2000: Retirement destination, 1990-00  
* Low_Employment_2015_update: Low employment counties, 2015 edition
* Population_loss_2015_update: Population loss counties, 2015 edition
* Retirement_Destination_2015_Update: Retirement destination counties, 2015 edition

# Appendix 2: Data cleaning {-}

The raw data sets are dirty and need transforming before we can do our EDA.
It takes time and efforts to clean and merge different data sources so we provide the final output of the cleaned and merged data. The cleaning procedure is as follows. Please read through to understand what is in the cleaned data. We set `eval = data_cleaned` in the following cleaning chunks so that these cleaning chunks will only run if any of `data/covid_county.csv`, `data/covid_rates.csv` or `data/covid_intervention.csv`  does not exist.

```{r}
# Indicator to check whether the data files exist
data_cleaned <- !(file.exists("data/covid_county.csv") & 
                    file.exists("data/covid_rates.csv") & 
                    file.exists("data/covid_intervention.csv"))
```

We first read in the table using `data.table::fread()`, as we did last time.

```{r Read in covid data, eval = data_cleaned}
# COVID case/mortality rate data
covid_rates <- fread("data/us_counties.csv", na.strings = c("NA", "", "."))
nyc <- fread("data/nycdata.csv", na.strings = c("NA", "", "."))

# Socioeconomic data
income <- fread("data/income.csv", na.strings = c("NA", "", "."))
jobs <- fread("data/jobs.csv", na.strings = c("NA", "", "."))
people <- fread("data/people.csv", na.strings = c("NA", "", "."))
county_class <- fread("data/county_classifications.csv", na.strings = c("NA", "", "."))

# Internvention policy data
int_dates <- fread("data/intervention_dates.csv", na.strings = c("NA", "", "."))
```

## Clean NYC data

The original NYC data contains more information than we need. We extract only the number of cases and deaths and format it the same as the `covid_rates` data.

```{r Clean NYC data, eval = data_cleaned}
# NYC county fips matching table 
nyc_fips <- data.table(FIPS = c('36005', '36047', '36061', '36081', '36085'),
                       County = c("BX", "BK", "MN", "QN", "SI"))

# nyc case
nyc_case <- nyc[,.(date = as.Date(date_of_interest, "%m/%d/%Y"),
                   BX = BX_CASE_COUNT, 
                   BK = BK_CASE_COUNT, 
                   MN = MN_CASE_COUNT, 
                   QN = QN_CASE_COUNT, 
                   SI = SI_CASE_COUNT)]

nyc_case %<>% 
  pivot_longer(cols = BX:SI, 
               names_to = "County",
               values_to = "cases") %>%
  arrange(date) %>%
  group_by(County) %>%
  mutate(cum_cases = cumsum(cases))

# nyc death
nyc_death <- nyc[,.(date = as.Date(date_of_interest, "%m/%d/%Y"),
                   BX = BX_DEATH_COUNT, 
                   BK = BK_DEATH_COUNT, 
                   MN = MN_DEATH_COUNT, 
                   QN = QN_DEATH_COUNT, 
                   SI = SI_DEATH_COUNT)]

nyc_death %<>% 
  pivot_longer(cols = BX:SI, 
               names_to = "County",
               values_to = "deaths") %>%
  arrange(date) %>%
  group_by(County) %>%
  mutate(cum_deaths = cumsum(deaths))

nyc_rates <- merge(nyc_case, 
                   nyc_death,
                   by = c("date", "County"),
                   all.x= T)

nyc_rates <- merge(nyc_rates,
                   nyc_fips,
                   by = "County")

nyc_rates$State <- "New York"
nyc_rates %<>% 
  select(date, FIPS, County, State, cum_cases, cum_deaths) %>%
  arrange(FIPS, date)
```
  
## Continental US cases

We only consider cases and death in continental US. Alaska, Hawaii, and Puerto Rico have 02, 15, and 72 as their respective first 2 digits of their FIPS. We use the `%/%` operator for integer division to get the first 2 digits of FIPS. We also remove Virgin Islands and Northern Mariana Islands. All data of counties in NYC are aggregated as `County == "New York City"` in `covid_rates` with no FIPS, so we combine the NYC data into `covid_rate`.

```{r, eval = data_cleaned}
covid_rates <- covid_rates %>% 
  arrange(fips, date) %>%
  filter(!(fips %/% 1000 %in% c(2, 15, 72))) %>%
  filter(county != "New York City") %>%
  filter(!(state %in% c("Virgin Islands", "Northern Mariana Islands"))) %>%
  rename(FIPS = "fips",
         County = "county",
         State = "state",
         cum_cases = "cases", 
         cum_deaths = "deaths")


covid_rates$date <- as.Date(covid_rates$date)

covid_rates <- rbind(covid_rates, 
                     nyc_rates)
```

## COVID date to week

We set the week of Jan 21, 2020 (the first case of COVID case in US) as the first week (2020-01-19 to 2020-01-25).

```{r, eval = data_cleaned}
covid_rates[, week := (interval("2020-01-19", date) %/% weeks(1)) + 1]
```

## COVID infection/mortality rates

Merge the `TotalPopEst2019` variable from the demographic data with `covid_rates` by FIPS.
  
```{r, eval = data_cleaned}
covid_rates <- merge(covid_rates[!is.na(FIPS)], 
                     people[,.(FIPS = as.character(FIPS),
                                   TotalPopEst2019)],
                     by = "FIPS",  
                     all.x = TRUE)
```


## NA in COVID data

NA values in the `covid_rates` data set correspond to a county not having confirmed cases/deaths. We replace the NA values in these columns with zeros. FIPS for Kansas city, Missouri, Rhode Island and some others are missing. We drop them for the moment and output the data up to week 57 as `covid_rates.csv`.

```{r, eval = data_cleaned}
covid_rates$cum_cases[is.na(covid_rates$cum_cases)] <- 0
covid_rates$cum_deaths[is.na(covid_rates$cum_deaths)] <- 0
```

```{r, eval = data_cleaned}
fwrite(covid_rates %>% 
         filter(week < 58) %>% 
         arrange(FIPS, date), 
       "data/covid_rates.csv")
```

## Formatting date in `int_dates`

We convert the columns representing dates in `int_dates` to R Date types using `as.Date()`. 
We will need to specify that the `origin` parameter is `"0001-01-01"`. 
We output the data as `covid_intervention.csv`.

```{r, eval = data_cleaned}
int_dates <- int_dates[-1,]
date_cols <- names(int_dates)[-(1:3)]
int_dates[, (date_cols) := lapply(.SD, as.Date, origin = "0001-01-01"), 
          .SDcols = date_cols]

fwrite(int_dates, "data/covid_intervention.csv")
```

## Merge demographic data

Merge the demographic data sets by FIPS and output as `covid_county.csv`.

```{r, eval = data_cleaned}
countydata <-
  merge(x = income,
        y = merge(
          x = people,
          y = jobs,
          by = c("FIPS", "State", "County")),
        by = c("FIPS", "State", "County"),
        all = TRUE)


countydata <- 
  merge(
    x = countydata,
    y = county_class %>% rename(FIPS = FIPStxt), 
    by = c("FIPS", "State", "County"),
    all = TRUE
  )

# Check dimensions
# They are now 3279 x 208
dim(countydata)
fwrite(countydata, "data/covid_county.csv")
```

