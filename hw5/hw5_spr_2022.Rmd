---
title: "Modern Data Mining - HW 5"
author:
- Diego G. Dávila
- Margaret Gardner
- Joelle Bagautdinova
date: 'Due: 11:59Pm,  4/10, 2022'
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.height=4, fig.width=6, warning = F, echo=F)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(randomForest, tree, ISLR, rpart, rattle, pROC, partykit, ggplot2, glmnet, leaps, dplyr, keras, neuralnet, imager, ranger, skimr, car, data.table, tm, GGally, RColorBrewer, wordcloud, parallel, MASS, doParallel, tidyselect, report)

```




# Overview

For the purpose of predictions, a model free approach could be beneficial. A binary decision tree is the simplest, still interpretable and often provides insightful information between predictors and responses. To improve the predictive power we would like to aggregate many equations, especially uncorrelated ones. One clever way to have many free samples is to take bootstrap samples. For each bootstrap sample we  build a random tree by taking a randomly chosen number of variables to be split at each node. We then take average of all the random bootstrap trees to have our final prediction equation. This is RandomForest. 

Ensemble method can be applied broadly: simply take average or weighted average of many different equations. This may beat any single equation in your hand.


All the methods covered can handle both continuous responses as well as categorical response with multiple levels (not limited to binary response.)


## Objectives


- Understand trees
    + single tree/displaying/pruning a tree
    + RandomForest
    + Ensemble idea

- R functions/Packages
    + `tree`, `RandomForest`, `ranger`
    
- Json data format

- text mining
    + bag of words
  

Data needed:

+ `IQ.Full.csv`
+ `yelp_review_20k.json`

# Problem 0: Lectures

Please study all three lectures. Understand the main elements in each lecture and be able to run and compile the lectures

+ textmining
+ trees
+ boosting




# Problem 1: IQ and successes

## Background: Measurement of Intelligence 

Case Study:  how intelligence relates to one's future successes?

**Data needed: `IQ.Full.csv`**

ASVAB (Armed Services Vocational Aptitude Battery) tests have been used as a screening test for those who want to join the army or other jobs. 

Our data set IQ.csv is a subset of individuals from the 1979 National Longitudinal Study of 
Youth (NLSY79) survey who were re-interviewed in 2006. Information about family, personal demographic such as gender, race and education level, plus a set of ASVAB (Armed Services Vocational Aptitude Battery) test scores are available. It is STILL used as a screening test for those who want to join the army! ASVAB scores were 1981 and income was 2005. 

**Our goals:** 

+ Is IQ related to one's successes measured by Income?
+ Is there evidence to show that Females are under-paid?
+ What are the best possible prediction models to predict future income? 


**The ASVAB has the following components:**

+ Science, Arith (Arithmetic reasoning), Word (Word knowledge), Parag (Paragraph comprehension), Numer (Numerical operation), Coding (Coding speed), Auto (Automative and Shop information), Math (Math knowledge), Mechanic (Mechanic Comprehension) and Elec (Electronic information).
+ AFQT (Armed Forces Qualifying Test) is a combination of Word, Parag, Math and Arith.
+ Note: Service Branch requirement: Army 31, Navy 35, Marines 31, Air Force 36, and Coast Guard 45,(out of 100 which is the max!) 

**The detailed variable definitions:**

Personal Demographic Variables: 

 * Race: 1 = Hispanic, 2 = Black, 3 = Not Hispanic or Black
 * Gender: a factor with levels "female" and "male"
 * Educ: years of education completed by 2006
 
Household Environment: 
 
* Imagazine: a variable taking on the value 1 if anyone in the respondent’s household regularly read
	magazines in 1979, otherwise 0
* Inewspaper: a variable taking on the value 1 if anyone in the respondent’s household regularly read
	newspapers in 1979, otherwise 0
* Ilibrary: a variable taking on the value 1 if anyone in the respondent’s household had a library card
	in 1979, otherwise 0
* MotherEd: mother’s years of education
* FatherEd: father’s years of education

Variables Related to ASVAB test Scores in 1981 (Proxy of IQ's)

* AFQT: percentile score on the AFQT intelligence test in 1981 
* Coding: score on the Coding Speed test in 1981
* Auto: score on the Automotive and Shop test in 1981
* Mechanic: score on the Mechanic test in 1981
* Elec: score on the Electronics Information test in 1981

* Science: score on the General Science test in 1981
* Math: score on the Math test in 1981
* Arith: score on the Arithmetic Reasoning test in 1981
* Word: score on the Word Knowledge Test in 1981
* Parag: score on the Paragraph Comprehension test in 1981
* Numer: score on the Numerical Operations test in 1981

Variable Related to Life Success in 2006

* Income2005: total annual income from wages and salary in 2005. We will use a natural log transformation over the income.


**Note: All the Esteem scores shouldn't be used as predictors to predict income**

## 1. EDA: Some cleaning work is needed to organize the data. 

+ The first variable is the label for each person. Take that out.
+ Set categorical variables as factors. 
+ Make log transformation for Income and take the original Income out
+ Take the last person out of the dataset and label it as **Michelle**. 
+ When needed, split data to three portions: training, testing and validation (70%/20%/10%)
  - training data: get a fit
  - testing data: find the best tuning parameters/best models
  - validation data: only used in your final model to report the accuracy. 


```{r}
# load data
df_full <- read.csv("data/IQ.Full.csv")

# data cleaning
df_full <- df_full %>% 
  # seet categorical variables as factors
  mutate(Race = as.factor(Race), 
         Gender = as.factor(Gender),
         Imagazine = as.factor(Imagazine),
         Inewspaper = as.factor(Inewspaper),
         Ilibrary = as.factor(Ilibrary),
         log_income = log(Income2005)
  )  %>%
  # remove ID, original income variable and Esteem variables as these should not be used as predictors
  dplyr::select(-c(Subject, Income2005), -contains(c("Esteem")))

# label last row as Michelle
Michelle <- tail(df_full, n = 1)

# remove the last row
df <- df_full %>%
  filter(row_number() <= n() - 1)

skim(df)

# split data into training, testing and validation
N <- dim(df)[1]
n1 <- floor(.7*N) # training = 70%
n2 <- floor(.2*N) # testing = 20%

set.seed(10)

# Split data to three portions of 70%/20%/10% of data size N
idx_train <- sample(N, n1)
idx_no_train <- (which(! seq(1:N) %in% idx_train))
idx_test <- sample(idx_no_train, n2)
idx_val <- which(! idx_no_train %in% idx_test)
data.train <- as.data.frame(df[idx_train,])
data.test <- as.data.frame(df[idx_test,])
data.val <- as.data.frame(df[idx_val,])

```


## 2. Factors affect Income

We only use linear models to answer the questions below.

i. To summarize ASVAB test scores, create PC1 and PC2 of 10 scores of ASVAB tests and label them as
ASVAB_PC1 and ASVAB_PC2. Give a quick interpretation of each ASVAB_PC1 and ASVAB_PC2 in terms of the original 10 tests. 

```{r}

# create ASVAB subset
ASVAB <- df[, 10:19]

# apply PCA with scaling and centering
ASVAB_pca <- prcomp(ASVAB, scale. = TRUE, center = TRUE)

# display the loadings
ASVAB_pca$rotation

# add PC1 and PC2 to the df
# df_pcs <- df
df$ASVAB_PC1 <- ASVAB_pca$x[,"PC1"]
df$ASVAB_PC2 <- ASVAB_pca$x[,"PC2"]

# # redo the same for the full dataset and reselect Michelle's row (having PC1 and PC2 for Michelle is necessary for the tree section below)
# ASVAB <- df_full[, 10:19]
# ASVAB_pca <- prcomp(ASVAB, scale. = TRUE, center = TRUE)
# df_full$ASVAB_PC1 <- ASVAB_pca$x[,"PC1"]
# df_full$ASVAB_PC2 <- ASVAB_pca$x[,"PC2"]
# 
# # label last row as Michelle once again (this time with the PCs)
# # Michelle <- tail(df_full, n = 1)

```


**Interpretation:** 

* ASVAB_PC1 appears to be a weighted sum of all 10 items and can thus be interpreted as a general ASVAB score. 
* ASVAB_PC2 appears to capture the difference between performance in more abstract reasoning (Arith, Word, Parag, Numer, Coding, Math) vs more applied tasks (Science, Auto, Mechanic, Elec). 

ii. Is there any evidence showing ASVAB test scores in terms of ASVAB_PC1 and ASVAB_PC2, might affect the Income?  Show your work here. You may control a few other variables, including gender. 

```{r}

# let's start with a simple linear model
fit1 <- lm(log_income ~ ASVAB_PC1 + ASVAB_PC2, data = df)
summary(fit1)

fit2 <- lm(log_income ~ ASVAB_PC1 + ASVAB_PC2 + Gender, data = df)
summary(fit2)

fit3 <- lm(log_income ~ ASVAB_PC1 + ASVAB_PC2 + Gender + Race, data = df)
Anova(fit3)

fit4 <- lm(log_income ~ ASVAB_PC1 + ASVAB_PC2 + Gender + Educ, data = df)
summary(fit4)

fit5 <- lm(log_income ~ ASVAB_PC1 + ASVAB_PC2 + Gender + Educ + MotherEd + FatherEd, data = df)
summary(fit5)
```

**Answer:** Based on the linear models above, the following observations can be made: 

* ASVAB_PC1 and ASVAB_PC2 both have a significant effect on log_outcome, where having a higher score on either PC is associated with higher income. 
* Gender has a significant effect, with males earning a higher income compared to females. 
* Years of education have a significant effect on log_income, where higher years of education for the individual or their father is associated with higher log_income. 
* Race or years of the mother's or father's education do not significantly impact log_income.

iii. Is there any evidence to show that there is gender bias against either male or female in terms of income in the above model? 

```{r}

# display results of model fit including Gender
Anova(fit2)
summary(fit2)

# testing the difference between the reduced model (without Gender) and full model (including Gender)
anova(fit1, fit2)
```

**Answer:** Based on the models above, we note that Gender does have a significant effect on log_income. Specifically, males tend to earn 10.097 + 0.67 more log_income compared to females (reference level).  

We next build a few models for the purpose of prediction using all the information available. From now on you may use the three data sets setting (training/testing/validation) when it is appropriate. 

## 3. Trees

i. fit1: tree(Income ~ Educ + Gender, data.train) with default set up 

    a) Display the tree
  
```{r}
# fit the tree
fit1 <- tree(log_income ~ Educ + Gender, data.train)

# view numerical output
summary(fit1)

# plot the tree
plot(fit1)
title(main="log_income ~ Educ + Gender")
text(fit1)

```

    
    b) How many end nodes? Briefly explain how the estimation is obtained in each end nodes and describe the prediction equation
    
**Answer:** The tree has 4 end nodes, meaning that Gender and Education are partitioned into 4 boxes. The end node estimations correspond to the sample mean log_income for every end node. The prediction equation for this model is as follows: 

* If gender == Female and years of education less than 15.5, the average log_income is 9.941
* If gender == Female and years of education more than 15.5, the average log_income is 10.410
* If gender == Male and years of education less than 15.5, the average log_income is 10.530
* If gender == Male and years of education more than 15.5, the average log_income is 11.170

    c) Does it show interaction effect of Gender and Educ over Income?
    
**Answer:** Yes, the tree shows an interaction between gender and income, where being male with more than 15.5 years of education corresponds to the highest log_income, and conversely being female with less than 15.5 years of education is associated with the lowest log_income. 

    d) Predict Michelle's income

```{r}

# predict Michelle's log income based on fit1
fit.Michelle <- predict(fit1, Michelle)
# fit.Michelle # the fitted salary in log scale 
data.frame(fitted=fit.Michelle,  obsy = Michelle["log_income"]) # compare with observed income for Michelle

Michelle_pred_income <- exp(fit.Michelle) # get Michelle's income


```

**Answer:** Michelle's predicted log_income is `r round(fit.Michelle,3)`, which corresponds to $ `r round(Michelle_pred_income,3)`. 

ii. fit2: fit2 <- rpart(Income2005 ~., data.train, minsplit=20, cp=.009)

    a) Display the tree using plot(as.party(fit2), main="Final Tree with Rpart") 
    
```{r, fig.width=9, fig.height=7}

# fit tree using rpart
fit2 <- rpart(log_income ~., data.train, minsplit = 20, cp = .009)

# plot the rpart tree
plot(as.party(fit2), main = "Final Tree with Rpart") 

```

    b) A brief summary of the fit2

**Answer:** This full tree including all predictors clearly illustrates the effect of gender on log_outcome, where being female leads to a lower income. The next splits involve several ASVAB scores (Math, Arith) and years of education. Overall, males with a high score in Arithmetic and at least 15.5 years of education are predicted to have the highest income. On the other hand, females with a Math score under 10.5 are predicted to earn the lowest income. 

    c) Compare testing errors between fit1 and fit2. Is the training error from fit2 always less than that from fit1? Is the testing error from fit2 always smaller than that from fit1? 
    
```{r}

# training error
fit1.rss <- summary(fit1)$dev # fit1 RSS
fit2.rss <- sum(residuals(fit2)^2) # fit2 RSS (fit2 has a different output as rpart was used)

# testing error
test.error.fit1 <- sum((predict(fit1, data.test) - data.test$log_income)^2)
test.error.fit2 <- sum((predict(fit2, data.test) - data.test$log_income)^2)
```

**Answer:** The training error of fit2 (`r fit2.rss`) is smaller than the training error of fit1 (`r fit1.rss`). However, the testing error of fit2 (`r test.error.fit2`) is slightly larger than the testing error of fit1 (`r test.error.fit1`). While the training error always should be smaller for trees that include more variables, this does not necessarily hold true for the testing error, as this corresponds to the model fit on unseen data.

    d) You may prune the fit2 to get a tree with small testing error. 
    
```{r}

### testing a few pruning options ###

# first print the cps of the rpart object
# Note: cp = complexity parameter. Any split that does not decrease the overall lack of fit by a factor of cp is not attempted.
printcp(fit2)
plotcp(fit2) # plot the cps

# find the option with the smallest error (in xerror)
fit2$cptable[which.min(fit2$cptable[,"xerror"]),"CP"] # looks like 0.009 was already the optimal cp!

# let's try several of these cp options on the test data to find their testing error 

# cp = 0.07
fit2.p1 <- prune(fit2, cp = 0.07)  # prune fit2 using prune() from the rpart package (as the tree has been generated with rpart)
print(paste("cp = 0.07: ", round(sum((predict(fit2.p1, data.test) - data.test$log_income)^2), 3))) # testing error 

# cp = 0.033
fit2.p2 <- prune(fit2, cp = 0.033)  # prune fit2 using prune() from the rpart package (as the tree has been generated with rpart)
print(paste("cp = 0.033: ", round(sum((predict(fit2.p2, data.test) - data.test$log_income)^2), 3))) # testing error 

# cp = 0.02
fit2.p3 <- prune(fit2, cp = 0.02)  # prune fit2 using prune() from the rpart package (as the tree has been generated with rpart)
print(paste("cp = 0.02: ", round(sum((predict(fit2.p3, data.test) - data.test$log_income)^2), 3))) # testing error 

# cp = 0.014
fit2.p4 <- prune(fit2, cp = 0.014)  # prune fit2 using prune() from the rpart package (as the tree has been generated with rpart)
print(paste("cp = 0.014: ", round(sum((predict(fit2.p4, data.test) - data.test$log_income)^2), 3))) # testing error 

# cp = 0.011
fit2.p5 <- prune(fit2, cp = 0.011)  # prune fit2 using prune() from the rpart package (as the tree has been generated with rpart)
print(paste("cp = 0.011: ", round(sum((predict(fit2.p5, data.test) - data.test$log_income)^2), 3))) # testing error 

# cp = 0.0091
fit2.p6 <- prune(fit2, cp = 0.0091)  # prune fit2 using prune() from the rpart package (as the tree has been generated with rpart)
print(paste("cp = 0.0091: ", round(sum((predict(fit2.p6, data.test) - data.test$log_income)^2), 3))) # testing error 

```

  
**Answer:** As it turns out, while cp = 0.009 has the smallest training error, cp = 0.011 has the smallest testing error. Thus, let's build the pruned tree using this criterion: 

```{r, fig.width=9, fig.height=7}
# summary(fit2.pruned)
data.table(fit2.p5$frame)

#plot the best subtrees
par(mfrow=c(1, 2))
plot(fit2) # main tree
title(main = "Main tree")
text(fit2) # main tree
plot(fit2.p5)
title(main="Best pruned tree (cp = 0.011)")
text(fit2.p5) # pruned tree

```

  
iii. fit3: bag two trees

    a) Take 2 bootstrap training samples and build two trees using the 
    rpart(Income2005 ~., data.train.b, minsplit=20, cp=.009). Display both trees.
    
```{r, fig.width=9, fig.height=7}

# bootstrap tree 1 
par(mfrow=c(1, 2))
n=nrow(data.train)
set.seed(1)  
index1 <- sample(n, n, replace = TRUE)
df_b1 <- data.train[index1, ]  #  first bootstrap sample
boot.1 <- rpart(log_income ~., df_b1, minsplit = 20, cp = .009) 
plot(boot.1)
title(main = "First bootstrap tree")
text(boot.1, pretty = 0)

# bootstrap tree 2 
set.seed(2) # will be a different sample
index1 <- sample(n, n, replace = TRUE)
df_b2 <- data.train[index1, ]  # second bootstrap sample
boot.2 <- rpart(log_income ~., df_b2, minsplit = 20, cp = .009) 
plot(boot.2)
title(main = "Second bootstrap tree")
text(boot.2, pretty = 0)
```


    b) Explain how to get fitted values for Michelle by bagging the two trees obtained above. Do not use the predict(). 
    
```{r eval=FALSE}

# Normally, we would bag the two trees by averaging the two fitted equations and predict() the response for Michelle as follows
# fit.bag.predict <- (predict(boot.1, Michelle) + predict(boot.2, Michelle))/2 #bagging
# data.frame(fitted=fit.bag.predict,  obsy = Michelle["log_income"])

# Alternatively, bagging can also be done using a randomforest, by specifying that the number of trees = 2 and by using ALL 20 predictors (which is the equivalent of bagging and removes the randomness)
# set.seed(1)
# fit3 <- randomForest(log_income ~., data.train, mtry = 20, ntree = 2)
# 
# # predict Michelle based on the 2 bagged trees
# fit.Michelle.2 <- predict(fit3, Michelle)
# data.frame(fitted=fit.Michelle.2,  obsy = Michelle["log_income"])
# 
# # could potentially get Michelle by looking at the OOB for her - for that we need to add Michelle back into the data.train sample
# set.seed(1)
# data.train.Michelle <- rbind(data.train, Michelle)
# fit3.wMichelle <- randomForest(log_income ~., data.train.Michelle, mtry = 20, ntree = 2)
# fit3.wMichelle$predicted[1809]

# The `randomForest()` function can be used as an alternative to get fitted values for Michelle based on two bagged trees above without using the predict function. More specifically, to do bagging of 2 trees, `ntree` has to be set to 2 and `mtry` has to be set to all included predictors (20 here), as this will lead to bagging instead of random tree building. Next, to get Michelle's fitted value without using predict(), we can add Michelle's data to the data.train sample and rerun the random forest. Her fitted log_income can then be extracted by selecting the last (newly added) row of `fit3.wMichelle$predicted[1809]`.
```

```{r}
# show Michelle's relevant variables
Michelle
```

**Answer:** Michelle's fitted values can be obtained without the predict() function by manually going through each tree node according to Michelle's data. To obtain the fitted values of the two bagged trees, we than compute the average of the fitted values corresponding to Michelle's category.
Thus: 

* Manually find the boot.1 fitted value for Michelle: 
  + Arith is 9 < 15.5
  + MotherEd is 9 < 9.5
  + Numer is 24 < 36.5
  + Coding is 27 < 40.5
  
  According to boot.1, Michelle has a predicted log_income of 9.545

* Manually find the boot.2 fitted value for Michelle: 
  + Gender == Female
  + Education is 13 < 17.5
  + Elec is 9 > 7.5
  
  According to boot.2, Michelle has a predicted log_income of 10.1
  
* Average the 2 fitted values to get Michelle's bagged fitted log_income: `r (9.545 + 10.1) / 2`


    c) What is the testing error for the bagged tree. Is it guaranteed that the testing error by bagging the two tree always smaller that either single tree? 
    
```{r}
# getting bagged tree testing error
fit3 <- as.vector((predict(boot.1, data.train) + predict(boot.2, data.train))/2) #bagging
mse <- mean((data.test$log_income - fit3)^2, na.rm = TRUE) # there are some NAs in the predicted values

print(paste("fit3 testing error: ", round(mse, 3)))
```

**Answer:** While generally bagging two trees will very likely tend to reduce the testing error, this does not necessarily hold rue when bbagging just two trees. More specifically, if one of the trees happen to have a large testing error, the resulting bagged testing error will be afffected and may be larger than the testing error of the "better" other input tree. 

    
iv. fit4: Build a best possible RandomForest

    a) Show the process how you tune mtry and number of trees. Give a very high level explanation how fit4 is built.
    
```{r}
# tune ntree
fit4 <- randomForest(log_income ~., data.train, mtry = 5, ntree = 500)    # change ntree
plot(fit4, col = "red", pch = 16, type = "p", 
     main="Tuning the number of trees")

# tune mtry
fit4.error.p <- 1:20  # set up a vector of length 20
for (p in 1:20)  # repeat the following code inside { } 20 times
{
  fit4 <- randomForest(log_income ~., data.train, mtry = p, ntree = 200) # loop over p from 1 to 20
  #plot(fit.rf, col= p, lwd = 3)
  fit4.error.p[p] <- fit4$mse[200]  # collecting oob mse based on 250 trees
}
# fit4.error.p   # oob mse returned: should be a vector of 20

plot(1:20, fit4.error.p, pch = 16,
     main = "Testing errors of mtry with 200 trees",
     xlab="mtry",
     ylab="OOB mse of mtry")
lines(1:20, fit4.error.p)

# final model (ntree = 200, mtry = 5)
fit4.final <- randomForest(log_income ~., data.train, mtry = 6, ntree = 200)
plot(fit4.final)

```

**Answer:** 

* First, we start by defining the optimal number of trees to be built (`ntree`). based on the first plot ("Tuning the number of trees"), we determine that 200 is likely a sufficient number of trees to have low OOB testing errors
* Next, we fix `ntree` = 200 and proceed with tuning `mtry`, which is the number of predictors, by looping and generating random forests for 1 up to 20 included predictors. We then plot the `OOB mse[200]` to compare them and find the best `mtry`. Based on the second plot ("Testing error of mtry with 200 trees"), it seems that the best number of predictors is around 4-6, which matches the generally recommended number of predictors (`mtry` = p/3 = 20/3 = 6.6). 
* The final fit4 model is thus built using `ntree = 200` and `mtry = 6`.


    b) Compare the oob errors from fit4 to the testing errors using your testing data. Are you convinced that oob errors estimate testing error reasonably well.
    

```{r}

# generate the testing errors
fit4.final.testing <- randomForest(log_income ~., data.train, xtest = data.test[, -20],
                          ytest = data.test[,20], mtry = 5, ntree = 200)
head(fit4.final.testing$mse)

# plotting the OOB errors and testing errors together
plot(1:200, fit4.final.testing$mse, col = "red", pch = 16,
     xlab = "number of trees",
     ylab = "mse",
     main = "mse's of RF: blue = OOB errors, red = testing errors")
points(1:200, fit4.final$mse, col = "blue", pch = 16)

```


**Answer:** Overall, testing error using the test data is very similar to the error obtained using OOB. Thus, OOB errors estimate the testing error reasonably well. 


    c) What is the predicted value for Michelle?


```{r}
# generating predicted value for Michelle based on fit4.final
fit4.final.Michelle <- predict(fit4.final, Michelle)
data.frame(fitted=fit.Michelle,  obsy = Michelle["log_income"]) # compare with observed income for Michelle
Michelle.fit.4_pred_income <- exp(fit4.final.Michelle)

```

**Answer:** Michelle's predicted log_income is `r fit4.final.Michelle`, which corresponds to $ `r Michelle.fit.4_pred_income`. 

   
v. Now you have built so many predicted models (fit1 through fit4 in this section). What about build a fit5 which bags fit1 through fit4. Does fit5 have the smallest testing error?

```{r}
# creating fit 5 by bagging fit1-4
fit5 <- (predict(fit1, data.train) + predict(fit2, data.train) + fit3 + predict(fit4.final, data.train))/4 #bagging

# show testing error from all created trees
# fit1
fit1.mse_oob <- mean((data.test$log_income - predict(fit1, data.test))^2)
print(paste("fit1 testing error: ", round(fit1.mse_oob, 3)))
# fit2
fit2.mse_oob <- mean((data.test$log_income - predict(fit2.p5, data.test))^2)
print(paste("fit2 testing error: ", round(fit2.mse_oob, 3)))
# fit3
fit3.mse_oob <- mean((data.test$log_income - fit3)^2)
print(paste("fit3 testing error: ", round(fit3.mse_oob, 3)))
# fit4
fit4.mse_oob <- mean((data.test$log_income - predict(fit4.final, data.test))^2)
print(paste("fit4 testing error: ", round(fit4.mse_oob, 3)))
# fit5 (bagged tree)
fit5.mse_oob <- mean((data.test$log_income - fit5)^2)
print(paste("fit5 testing error: ", round(fit5.mse_oob, 3)))

# NB: RSS (residual sum of squares) is a part of MSE (mean squared error): MSE = RSS/ degree of freedom. Essentially for RSS we do sum() and for MSE we do mean()

```

**Answer:** fit5 does not have the smallest testing error.


vi.  Summarize the results and nail down one best possible final model you will recommend to predict income. Explain briefly why this is the best choice. Finally for the first time evaluate the prediction error using the validating data set. 

```{r}

# creating fit 6 by bagging fit1, 2 and 4 (exclude fit3 due to its high testing error)
fit6 <- (predict(fit1, data.train) + predict(fit2, data.train) + predict(fit4.final, data.train))/3 #bagging

# fit6 (bagged tree)
fit6.mse_oob <- mean((data.test$log_income - fit6)^2)
print(paste("fit5 testing error: ", round(fit6.mse_oob, 3)))

# the testing error of fit6 is lower than fit5, but still doesn't beat fit4.final. Therefore, we'll stick with fit4 final as the best possible final model to predict log_outcome. 

# prediction error using validation data
fit4.mse_oob_validation <- mean((data.val$log_income - predict(fit4.final, data.val))^2)
print(paste("fit4 prediction error using the validation data set: ", round(fit4.mse_oob_validation, 3)))

# show variables used in the random forest model
# vars <- fit4.final$importance[,1, drop = FALSE]
# row.names(vars)

```


**Answer:** Based on the comparison of testing errors shown in the previous section (v), it appeared that fit5 had a larger testing error than fit1, fit2 and fit4.final. However, given that fit3 had the largest testing error overall and thus reflected a poor model fit, we created a fit6 tree by bagging fit1, fit2 and fit4.final, excluding fit3 to determine whether this would reduce the testing error. This changed the testing error from `r round(fit5.mse_oob, 3)` to `r round(fit6.mse_oob, 3)`, making it still larger than the error for fit4.final (`r  round(fit4.mse_oob, 3)`). Thus, we are keeping fit4.final as the best model to predict income because it has the smallest testing error. The prediction error using the validation data set is even lower (`r round(fit4.mse_oob_validation, 3)`), confirming that this is a good model for the prediction of log income.  


vii. Use your final model to predict Michelle's income. 

```{r}
fit4.final.Michelle <- predict(fit4.final, Michelle)
data.frame(fitted=fit.Michelle,  obsy = Michelle["log_income"]) # compare with observed income for Michelle

Michelle.fit.4_pred_income <- exp(fit4.final.Michelle)

```

**Answer:** Michelle's predicted log_income is `r fit4.final.Michelle`, which corresponds to $ `r Michelle.fit.4_pred_income`.

    
# Problem 2: Yelp challenge 2019

**Note:** This problem is rather involved. It covers essentially all the main materials we have done so far in this semester. It could be thought as a guideline for your final project if you want when appropriate. 

Yelp has made their data available to public and launched Yelp challenge. [More information](https://www.yelp.com/dataset/). It is unlikely we will win the $5,000 prize posted but we get to use their data for free. We have done a detailed analysis in our lecture. This exercise is designed for you to get hands on the whole process. 

For this case study, we downloaded the [data](https://www.yelp.com/dataset/download) and took a 20k subset from **review.json**. *json* is another format for data. It is flexible and commonly-used for websites. Each item/subject/sample is contained in a brace *{}*. Data is stored as **key-value** pairs inside the brace. *Key* is the counterpart of column name in *csv* and *value* is the content/data. Both *key* and *value* are quoted. Each pair is separated by a comma. The following is an example of one item/subject/sample.

```{json, eval=F}
{
  "key1": "value1",
  "key2": "value2"
}
```


**Data needed:** yelp_review_20k.json available in Canvas.

**yelp_review_20k.json** contains full review text data including the user_id that wrote the review and the business_id the review is written for. Here's an example of one review.

```{json, eval=FALSE}
{
    // string, 22 character unique review id
    "review_id": "zdSx_SD6obEhz9VrW9uAWA",

    // string, 22 character unique user id, maps to the user in user.json
    "user_id": "Ha3iJu77CxlrFm-vQRs_8g",

    // string, 22 character business id, maps to business in business.json
    "business_id": "tnhfDv5Il8EaGSXZGiuQGg",

    // integer, star rating
    "stars": 4,

    // string, date formatted YYYY-MM-DD
    "date": "2016-03-09",

    // string, the review itself
    "text": "Great place to hang out after work: the prices are decent, and the ambience is fun. It's a bit loud, but very lively. The staff is friendly, and the food is good. They have a good selection of drinks.",

    // integer, number of useful votes received
    "useful": 0,

    // integer, number of funny votes received
    "funny": 0,

    // integer, number of cool votes received
    "cool": 0
}
```

## Goal of the study

The goals are 

1) Try to identify important words associated with positive ratings and negative ratings. Collectively we have a sentiment analysis.  

2) To predict ratings using different methods. 

## 1. JSON data and preprocessing data

i. Load *json* data

The *json* data provided is formatted as newline delimited JSON (ndjson). It is relatively new and useful for streaming.
```{json, eval=F}
{
  "key1": "value1",
  "key2": "value2"
}
{
  "key1": "value1",
  "key2": "value2"
}
```

The traditional JSON format is as follows.
```{json, eval=F}
[{
  "key1": "value1",
  "key2": "value2"
},
{
  "key1": "value1",
  "key2": "value2"
}]
```


We use `stream_in()` in the `jsonlite` package to load the JSON data (of ndjson format) as `data.frame`. (For the traditional JSON file, use `fromJSON()` function.)

```{r}
pacman::p_load(jsonlite)
yelp_data <- jsonlite::stream_in(file("data/yelp_review_20k.json"), verbose = F)
str(yelp_data)  

# different JSON format
# tmp_json <- toJSON(yelp_data[1:10,])
# fromJSON(tmp_json)
```

**Write a brief summary about the data:**

a) Which time period were the reviews collected in this data?

```{r}
yelp_data %>% 
  summarise(start.date = as.Date(min(date)),
            end.date = as.Date(max(date)))
```
The reviews were written between Oct. 19th, 2004 and Oct. 4th, 2018. 

b) Are ratings (with 5 levels) related to month of the year or days of the week? Only address this through EDA please. 

```{r}

month.of.year <- months(as.Date(yelp_data$date)) # extract the months
day.of.week <- weekdays(as.Date(yelp_data$date)) # extract the day

print("Proportion Table: Days of Week - Within a Day")
prop.table(table(yelp_data$stars, day.of.week), 2)  # create a proportion table for the weekdays (prop across cols, on day X what proportion were Y stars)
print("Proportion Table: Days of Week - Within a Star")
prop.table(table(yelp_data$stars, day.of.week), 1)  # create a proportion table for the weekdays (prop across rows, of Y star reviews, what proportion were on X day)

print("Proportion Table: Month of Year - Within a Month")
prop.table(table(yelp_data$stars, month.of.year), 2)  # create a proportion table for the weekdays (prop across cols, on month X what proportion were Y starts)
print("Proportion Table: Month of Year - Within a Star")
prop.table(table(yelp_data$stars, month.of.year), 1)  # create a proportion table for the weekdays (prop across rows, of Y star reviews, what proportion were on X month)


#messing around with plots
days <- c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")
yelp_data <- yelp_data %>% mutate(month=factor(months(as.Date(yelp_data$date)), levels=month.name),
                     weekday=factor(weekdays(as.Date(yelp_data$date)), levels=days)) #adding month and weekday cols

ggplot(yelp_data, aes(fill=as.factor(stars), x=month)) + 
  geom_bar(position="stack", stat="count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title="Ratings by Month", fill="stars")

ggplot(yelp_data, aes(fill=as.factor(stars), x=weekday)) + 
  geom_bar(position="stack", stat="count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title="Ratings by Day of the Week", fill="stars")
```

It appears based on the above proportion tables that there is no major relationship between day of the week or month of the year and review score. 

ii. Document term matrix (dtm)
 
 Extract document term matrix for texts to keep words appearing at least .5% of the time among all 20000 documents. Go through the similar process of cleansing as we did in the lecture. 

```{r}
just.text <- yelp_data$text # get just the review text
full.corpus <- VCorpus(VectorSource(just.text)) # convert to corpus
# create controls for data processing
clist <- list(tolower = TRUE, 
                    removePunctuation = TRUE,
                    removeNumbers = TRUE, 
                    stopwords = stopwords("english"), 
                    stemming = TRUE)

doc.term.matrix.full <- DocumentTermMatrix(full.corpus, control = clist) # create the matrix
doc.term.matrix <- removeSparseTerms(doc.term.matrix.full, 1-.005)  # remove words that appear in fewer than 0.5 percent reviews
inspect(doc.term.matrix)
```

a) Briefly explain what does this matrix record? What is the cell number at row 100 and column 405? What does it represent?

This matrix records the amount of times that a particular word appears in a given review. 

```{r}
inspect(doc.term.matrix[100, 405])
```

At row 100, column 405, we have a number 0. This corresponds to the amount of times that a particular word (the word in column 405) appears in document 100. 

b) What is the sparsity of the dtm obtained here? What does that mean?

The sparsity of the matrix is 97% here, meaning that 97% of the elements are zero. This means that 97% of the time a given word does not appear in a given document. 

iii. Set the stars as a two category response variable called rating to be “1” = 5,4 and “0”= 1,2,3. Combine the variable rating with the dtm as a data frame called data2. 

```{r}
# create rating variable
rating <- yelp_data %>%
  transmute(rating = as.factor(ifelse(stars == 5 | stars == 4, 1, 0)) )
# make data2 with the dtm and rating
data2 <- data.frame(rating, as.matrix(doc.term.matrix)) 
data2[1:10,1:25] # view part of dataframe
```

## Analysis

Get a training data with 13000 reviews and the 5000 reserved as the testing data. Keep the rest (2000) as our validation data set. 

```{r}
set.seed(1) # for reproducibility
train.idx <- sample(nrow(data2), 13000) # extract 13000 reviews to use as training data
test.idx <- sample(seq(1, nrow(data2))[-train.idx], 5000) # extract 5000 reviews to use as testing data that were not in training data
validation.idx <- sample(seq(1, nrow(data2))[-c(train.idx, test.idx)], nrow(data2) - 13000 - 5000) # extract remaining rows as validation data
data2.test <- data2[test.idx,] # create test data
data2.train <- data2[train.idx,] # create training data
data2.validation <- data2[validation.idx,] # create validation data
```

## 2. LASSO

i. Use the training data to get Lasso fit. Choose lambda.1se. Keep the result here.

```{r}
Y <- data2.train[, 1] # extract Y
X <- sparse.model.matrix(rating~., data=data2.train)[, -1] # get design matrix without the first col of 1's
set.seed(1)  # to control the randomness in K folds 
fit.lasso <- cv.glmnet(X, Y, alpha=1, nfolds=10, family="binomial")
plot(fit.lasso) #plot lambdas

coef.1se <- coef(fit.lasso, s="lambda.1se") #variables ID'd 
lasso.words <- coef.1se@Dimnames[[1]] [coef.1se@i][-1] # non-zero variables without intercept. 
```

ii. Feed the output from Lasso above, get a logistic regression. 

```{r}
sel_cols <- c("rating", lasso.words) #select y and LASSO variables
data2.train_sub <- data2.train %>% dplyr::select(all_of(sel_cols)) #subset data
fit.glm <- glm(rating~., family=binomial, data2.train_sub) #run glm
```

a) Pull out all the positive coefficients and the corresponding words. Rank the coefficients in a decreasing order. Report the leading 2 words and the coefficients. Describe briefly the interpretation for those two coefficients. 

```{r}
fit.glm.coef <-coef(fit.glm) #pull logistic coef
good.glm <- fit.glm.coef[which(fit.glm.coef > 0)] #positive words
good.glm <- good.glm[-1] %>% #remove intercept
  sort(decreasing=TRUE) %>% #sort
  round(4)
good.glm[1:2] #leading 2 words
```

Among all the words used in the sampled reviews, "thorough" and "delight" are most predictive of a high rating. For each time that "thorough" or "delight" are used in a review, the log(odds) of the rating being 4 or 5 stars increases by about `r as.numeric(good.glm[1])` and `r as.numeric(good.glm[2])`, respectively.

b) Make a word cloud with the top 100 positive words according to their coefficients. Interpret the cloud briefly.

```{r, message=FALSE, warning=FALSE, results= TRUE}
cor.special <- brewer.pal(8,"Dark2")  #set color scheme
wordcloud(names(good.glm[1:100]), good.glm[1:100],  # make a word cloud
          colors=cor.special, ordered.colors=F)
```
Each word displayed is predictive of a higher rating (4 or 5 stars), with size corresponding to the strength of that association. Many of these word stems ("pleasur", "delight", "comfort", etc.) have strong positive connotations. However, some stems, like "slight", "thursday", or "worri", are less intuitively positive, indicating that the associations identified by the model implicitly account for the context in which the stem appears. In other words, the model is able to identify "worri" as predicting positive ratings, suggesting that many positive reviews include phrases such as "no worries", "never worry" or similar. 

c) Repeat i) and ii) for the bag of negative words.

```{r}
bad.glm <- fit.glm.coef[which(fit.glm.coef < 0)] #negative words
bad.glm <-sort(-bad.glm, decreasing = TRUE) %>%
  round(4)
bad.glm[1:2] #leading 2 words
```

Out of all words stems to appear in the reviews, "wors" and "rude" are most predictive of a low (3 or fewer star) rating. For each time that "wors" or "rude" are used in a review, the log(odds) of the rating being 4 or 5 stars decreases by about `r as.numeric(bad.glm[1])` and `r as.numeric(bad.glm[2])`, respectively.

```{r, message=FALSE, warning=FALSE, results= TRUE}
wordcloud(names(bad.glm[1:100]), bad.glm[1:100],  # make a word cloud
          colors=cor.special, ordered.colors=F)
```

The words most strongly predictive of a negative review are those that suggest unpleasant interactions and/or food quality, such as "rude", "terribl", "overcook" and "gross". Interestingly, both "woman" and "women" are identified as being associated with lower ratings; though further analysis would be necessary to determine the root of this association, it may reflect differing attitudes toward and expectations of women in customer service roles.

d) Summarize the findings. 

iii. Using majority votes find the testing errors
	i) From Lasso fit in 3)
```{r}
predict.lasso <- predict(fit.lasso, as.matrix(data2.test[, -1]), type = "class", s="lambda.1se") #output majority vote labels
testerror.lasso <- mean(data2.test$rating != predict.lasso) #testing error
```

	ii) From logistic regression in 4)
```{r}
predict.glm <- predict(fit.glm, data2.test, type = "response") #prediction for testing data
class.glm <- ifelse(predict.glm > .5, "1", "0")

testerror.glm <- mean(data2.test$rating != class.glm) #find prediction error for data.test
```
	
	iii) Which one is smaller?
	
The testing error for the LASSO fit (`r mean(data2.test$rating != predict.lasso)`) is smaller than that of the logistic regression (`r testerror.glm`).

## 3. Random Forest  

i. Briefly summarize the method of Random Forest

Random Forest methods consist of leveraging multiple decision trees to solve classification or regression problems. Each tree is built recursively as a series of split-points, each optimized from among a set of variables to minimize misclassification error (for classification) or residual sum of squares (for regression). In Random Forest, some number of such decision trees are initialized, each using a subset of the available variables, and a bootstrapped sampled of the data. In the case of a regression problem, the aggregated output is typically an average of all predictions based on each decision tree. In the case of a classification problem, the aggregated output is typically a majority vote, based on the classification by each decision tree.

ii. Now train the data using the training data set by RF. Get the testing error of majority vote. Also explain how you tune the tuning parameters (`mtry` and `ntree`). 

```{r}
set.seed(1) # for reproducibility
# first we tune the parameters
rf.fit1 <- randomForest(rating~., data2.train, mtry=6, ntree=250)
plot(rf.fit1, main = "Testing Error by Number of Trees")
legend("topright", colnames(rf.fit1$err.rate), col=1:3, cex=0.8, fill=1:3)
```

In order to tune the ntree parameter, we have plotted the Out of Bag classification error against the number of trees used, with mtry held constant. Based on this plot, 100-150 trees seem to be appropriate, as the decrease in error is negligible past this point. Accordingly, we opted to use 150 trees. 

Now, we will focus on optimizing mtry. To do this, we will do randomForest over a range of mtry values, with ntree held constant, and plot the OOB classification error for each value. Because this process will be computationally expensive, I have run my code with parallel processing and saved the output to mse_mtry.Rdata, so as to not have to re-run this chunk. 

```{r, eval=FALSE}
#max.vars <- dim(data2.train)[2] - 1 # max number of variables (subtract 1 to account for the ratings variable)
# this will take a LONG time on its own, so we'll implement parallel processing for our loop. 

numCores <- detectCores() # detect the number of available cores. 
cluster1 <- makeCluster(numCores-2, # number of cores to use, let's leave 1 core free
                         type = "FORK") # type of cluster, FORK lets us use stuff already in session
registerDoParallel(cluster1) # register our cluster 

max.vars <- 60
error.vector <- foreach(x = 1:max.vars, .combine = 'c') %dopar% {
  temp.fit <- randomForest(rating~., data2.train, mtry=x, ntree=150)
  temp.fit$err.rate[150,1]
  }
stopCluster(cluster1) # close the cluster

save(error.vector, file = "mse_mtry.Rdata")
```


```{r}
max.vars <- 60
load("mse_mtry.Rdata")
# plot the MSE
plot(1:max.vars, error.vector, pch=16,
     main = "OOB Classification Error, 150 Trees",
     xlab="mtry parameter",
     ylab="OOB Error")
lines(1:max.vars, error.vector)
```

As we can see, the OOB classification error begins to plateau around 20 variables, so we can select this as our mtry. 

```{r}
rf.fit.final <- randomForest(rating~., data2.train, mtry=20, ntree=150) # fit the final random forest model
rf.fit.final$err.rate[150, 1]

predict.rf <- predict(rf.fit.final, data2.test)
test.data.error <- mean(data2.test$rating != predict.rf) 
```
The testing error of majority vote is: `r test.data.error`. 

## 4. Boosting - REMOVED

SECTION REMOVED


## 5.  PCA first

i. Perform PCA (better to do sparse PCA) for the input matrix first. Decide how many PC's you may want to take and why.

```{r}
set.seed(1)
# using sparse matrix (terms appear in at least 0.5% reviews)
terms.df <- data2.train[,-1] #remove rating
pc.terms<- prcomp(terms.df, scale=TRUE)
```

```{r}
pc.loading <- pc.terms$rotation[,1:50]
#knitr::kable(pc.loading) #look at loadings for top 50 PCs

#scree plots
plot(summary(pc.terms)$importance[2,], 
     ylab="PVE",
     xlab="Number of PC's",
     pch = 16, 
     main="Scree Plot of PVE")

plot(summary(pc.terms)$importance[3, ], pch=16,
     ylab="Cumulative PVE",
     xlab="Number of PC's",
     main="Scree Plot of Cumulative PVE")
```

Using 100 PCs based on where PVE levels off in our scree plot.

ii. Pick up one of your favorite method above and build the predictive model with PC's.

Using relaxed LASSO since it had a smaller testing error on the original data.

```{r}
loadings.df <- data.frame(data2.train[, 1], pc.terms$x[,1:100]) %>% # add rating back
  rename(rating = 1)

Y <- loadings.df[, 1] 
X <- sparse.model.matrix(rating~., data=loadings.df)[, -1] # get design matrix without the first col of 1's
set.seed(1)  # to control the randomness in K folds 
fit.pca <- cv.glmnet(X, Y, alpha=1, nfolds=10, family="binomial")
plot(fit.pca) #plot lambdas

pc.coef.1se <- coef(fit.pca, s="lambda.1se") #variables ID'd 
pc.coef <- pc.coef.1se@Dimnames[[1]] [pc.coef.1se@i][-1] # non-zero variables without intercept
```

iii. What is the testing error? Is this testing error better than that obtained using the original x's? 

```{r}
pc.test <- predict(pc.terms, data2.test[,-1]) #apply PC loadings from train to test
predict.pc.lasso <- predict(fit.pca, pc.test[,1:100], type = "class", s="lambda.1se") #output majority vote labels
pc.test.er <- mean(data2.test$rating != predict.pc.lasso) #testing error
```

The testing error of the LASSO fit modeled using the 100 leading PCs is 0.1362, almost identical for the testing error of the LASSO fit on the original data (0.1342).

## 6. Ensemble model

i. Take average of some of the  models built above (also try all of them) and this gives us the fifth model. Report it's testing error. (Do you have more models to be bagged, try it.)

```{r}
# create a function to find the mode. Taken from: https://stackoverflow.com/questions/2547402/how-to-find-the-statistical-mode
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

# we'll use majority vote
ensemble.model.predict.all <- data.frame(predict(fit.pca, pc.test[,1:100], type = "class", s="lambda.1se"),     # PCA + LASSO 
                                predict(rf.fit.final, data2.test),                                              # Random Forest
                                ifelse(predict(fit.glm, data2.test, type = "response") > .5, "1", "0"),         # Logistic Regression
                                predict(fit.lasso, as.matrix(data2.test[, -1]), type = "class", s="lambda.1se") # LASSO 
                           )
ensemble.model.predict.vote <- apply(ensemble.model.predict.all, 2, FUN = Mode)
ensemble.prediction.error <- mean(data2.test$rating != ensemble.model.predict.vote)
ensemble.prediction.error

# average probabilities
df.prob <- data2.test %>% dplyr::select(rating) #initialize df to store outputs
df.prob$pca <- predict(fit.pca, pc.test[,1:100], type = "response", s="lambda.1se") 
df.prob$rf <- predict(rf.fit.final, data2.test, type="prob")[,2] 
df.prob$glm <- predict(fit.glm, data2.test, type = "response") 
df.prob$lasso <- predict(fit.lasso, as.matrix(data2.test[, -1]), type = "response", s="lambda.1se") 
df.prob <- df.prob %>%
  mutate(en.class = ifelse(rowMeans(df.prob[ , c(2:5)], na.rm=FALSE) > .5, "1", "0")) # majority rule classification based on averaged probabilities 
en.prob.testerror <- mean(data2.test$rating != df.prob$en.class)

# reduced ensemble - removing glm, since it has the worst testing error
df.prob <- df.prob %>%
  mutate(en.class.noglm = ifelse(rowMeans(df.prob[ , c("pca", "rf", "lasso")], na.rm=FALSE) > .5, "1", "0")) # majority rule classification based on averaged probabilities 
prob.noglm.testerror <- mean(data2.test$rating != df.prob$en.class.noglm)
```

We created several ensemble models based on the Random Forest, LASSO, logistic regression, and PCA+LASSO models fitted above. First we classified reviews based on majority vote of the classifications assigned by each individual model (i.e. the mode of the predictions, similar to the approach used by RandomForest). This resulted in a prediction error of `r ensemble.prediction.error`; far worse than our best fitting model (LASSO). Next, we used each model to calculate the probability of a rating being 4 or above, averaged the probabilities across models, and used this average to classify based on a majority vote (threshold at probability >0.5). This improved our testing error to `r en.prob.testerror`. Finally, we ran the same calculation but without including the probability predicted by our lowest-performing model (logistic regression, testing error = `r testerror.glm`), resulting in a testing error of `r prob.noglm.testerror` for our final ensemble model.

## 7. Final model

Which classifier(s) seem to produce the least testing error? Are you surprised? Report the final model and accompany the validation error. Once again this is THE only time you use the validation data set.  For the purpose of prediction, comment on how would you predict a rating if you are given a review (not a tm output) using our final model? 

```{r}
# create a table of all models' testing errors
df.testerr <- data.frame(model=c('PCA+LASSO', 'RandomForest', 'LogisticReg', 'LASSO', 'EnMode', 'EnProb', 'EnProbReduced'),
                 test.err=c(pc.test.er, test.data.error, testerror.glm, testerror.lasso, ensemble.prediction.error, en.prob.testerror, prob.noglm.testerror))

df.testerr[which.min(df.testerr$test.err),] #return model with lowest testing error
```

Our Final Model: Average Probability Ensemble of LASSO, RandomForest, and PCA+LASSO

The model that produces the lowest testing error is the final ensemble model (majority vote classification based on probabilities averaged from LASSO, RandomForest, and PCA+LASSO). This is not terribly surprising, as the ensemble approach is able to minimize variance by averaging probabilities from our highest-performing models. However, this approach lacks interpretability and only slightly improves testing error beyond that of our LASSO model. 

```{r}
# first apply PC loadings from train to validation data for PC+LASSO prediction
pc.val <- predict(pc.terms, data2.validation[,-1])
# calculate average probabilities for validation data 
df.prob.val <- data2.validation %>% dplyr::select(rating) #initialize df to store outputs
df.prob.val$pca <- predict(fit.pca, pc.val[,1:100], type = "response", s="lambda.1se") 
df.prob.val$rf <- predict(rf.fit.final, data2.validation, type="prob")[,2] 
df.prob.val$lasso <- predict(fit.lasso, as.matrix(data2.validation[, -1]), type = "response", s="lambda.1se") 
# majority rule classification based on averaged probabilities 
df.prob.val <- df.prob.val %>%
  mutate(en.class.noglm = ifelse(rowMeans(df.prob.val[ , c("pca", "rf", "lasso")], na.rm=FALSE) > .5, "1", "0")) 
prob.validation.error <- mean(data2.validation$rating != df.prob.val$en.class.noglm) # test error on validation data
```

The validation error for our final model is `r prob.validation.error`. 

Were we tasked with making a prediction with this model, given a review, we would extract a dtm and then apply our classification model much as we did with the validation dataset. Specifically, we would:

1. Process the text. This involves cleaning procedures such as removing punctuation, stem words, etc. 
2. Generate word counts for each word that appears in the review as a data.frame.  
3. Order the words as appears in the data.frame we used to train our model. 
4. Convert this data frame to a 1D matrix. 
5. Predict probability of a positive rating on our LASSO model by feeding the dtm as input to the predict() function, using our fit.lasso as the model, with type='response', s='lambda.1se'. 
6. Predict probability of a positive rating on our Random Forest model by feeding the dtm as input to the predict() function, using our rf.fit.final as the model, with type='prob'. 
7. Predict probability of a positive rating on our PCA+LASSO model by applying our PC loadings to the dtm using predict() and then feeding the first 100 PCs to the predict() function, using our fit.lasso as the model, with type='response', s='lambda.1se'.
8. Average the predicted probabilities from steps 5-7 and classify the review based on majority vote (e.g. assign a "good" rating of 4-5 stars if probability >0.5, else assign a "bad" rating of 1-3 stars)

